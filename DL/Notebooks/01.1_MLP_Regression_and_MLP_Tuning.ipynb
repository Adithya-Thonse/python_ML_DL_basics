{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HklD4MlOgCTy"
      },
      "source": [
        "# MLP Regression and MLP Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doENGez0k03C"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbFv4sGHk7VW"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "- understand the concept of MLPs for regression\n",
        "- know the hyperparameters of neural network and their tuning \n",
        "- understand batch normalization using Keras\n",
        "- understand the concept of optimizers\n",
        "- understand the time-based learning rate method through an example\n",
        "- understand the different regularization methods to avoid the overfitting of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-9AxpS_WMr9"
      },
      "source": [
        "### Introduction to Regression MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btCQw9drWTH_"
      },
      "source": [
        "First, MLPs can be used for regression tasks. If we want to predict a single value (e.g., the  price  of  a  house  given  many  of  its  features), then  we  just  need  a  single  output neuron:  its  output  is  the  predicted  value.  For  multivariate  regression  (i.e.,  to  predict multiple  values  at  once),  we  need  one  output  neuron  per  output  dimension.  For example, to locate the center of an object on an image, we need to predict 2D coordinates,  so  we  need  two  output  neurons.  If  we  also  want  to  place  a  bounding  box around the object, then we need two more numbers: the width and the height of the object. So we end up with 4 output neurons.\n",
        "\n",
        "In general, when building an MLP for regression, we do not want to use any activation  function  for  the  output  neurons,  so  they  are  free  to  output  any  range  of  values. However,  if  we  want  to  guarantee  that  the  output  will  always  be  positive,  then  we can use the ReLU activation function or the softplus activation function in the output layer.  Finally,  if  we  want  to  guarantee  that  the  predictions  will  fall  within  a  given range of values, then we can use the logistic function or the hyperbolic tangent and scale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for the hyperbolic tangent.\n",
        "\n",
        "The loss function to use during training is typically the mean squared error, but if we have  a  lot  of  outliers  in  the  training  set,  we  may  prefer  to  use  the  mean  absolute error  instead.  Alternatively,  we  can  use  the  Huber  loss,  which  is  a  combination  of both.\n",
        "\n",
        "**Note:** The Huber loss is quadratic when the error is smaller than a threshold $δ$ (typically 1), but linear when the error is larger than $δ$. This makes it less sensitive to outliers than the mean squared error, and\n",
        "it  is  often  more  precise  and  converges  faster  than  the  mean  absolute error.\n",
        "\n",
        "To know more about Multi Layer Perceptron (MLP), click [here](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L05%20Multilayer%20Perceptrons.pdf).\n",
        "\n",
        "**Implementation Using Keras**\n",
        "\n",
        "Keras is a high-level Deep Learning API that allows us to easily build train, evaluate and execute all sorts of neural networks. To know more about the documentation of Keras, click [here](https://keras.io/).\n",
        "\n",
        "**Implementation of MLP regression Using sklearn**\n",
        "\n",
        "The very popular machine learning library Scikit-Learn is also capable of basic deep learning modeling.\n",
        "\n",
        "Salient points of Multilayer Perceptron (MLP) in Scikit-learn:\n",
        "\n",
        "* There is no activation function in the output layer.\n",
        "* For regression scenarios, the square error is the loss function, and cross-entropy is the loss function for the classification\n",
        "* It can work with single as well as multiple target values regression.\n",
        "* Unlike other popular packages, likes Keras the implementation of MLP in Scikit doesn’t support GPU.\n",
        "\n",
        "To know more about Scikit-Learn MLP regressor, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx6BJLwYZBxw"
      },
      "source": [
        "### Typical MLP Regressor Architecture"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GH2a3AvwZNlT"
      },
      "source": [
        "\n",
        "Hyperparameter             | Typical Value \n",
        "---------------------------|------------------\n",
        "input neurons            | One per input feature (e.g., 28 x 28 = 784 for MNIST)\n",
        "hidden layers            | Depends on the problem. Typically 1 to 5. \n",
        "neurons per hidden layer | Depends on the problem. Typically 10 to 100.\n",
        "output neurons           | 1 per prediction dimension\n",
        "Hidden activation          | ReLU\n",
        "Output activation          | None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\n",
        "Loss function              | MSE or MAE/Huber (if outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAu6v-CnA8zj"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SRN62EfayXM",
        "outputId": "6f1d365b-b1d2-4520-e563-1d621421cb17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 204 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 215 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 245 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 276 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 286 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 296 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 307 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 317 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 358 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 368 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 378 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 389 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 399 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 409 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 419 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 430 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 450 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 460 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 471 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 481 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 491 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 501 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 512 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 522 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 532 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 542 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 552 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 563 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 573 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 593 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 604 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 614 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 624 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 634 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 645 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 655 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 665 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 675 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 686 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 696 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 706 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 716 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 727 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 737 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 747 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 757 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 768 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 778 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 788 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 798 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 808 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 819 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 829 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 839 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 849 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 860 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 870 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 880 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 890 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 901 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 911 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 921 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 931 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 942 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 952 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 962 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 972 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 983 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 993 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 7.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# install livelossplot package to visualize epoch by epoch loss and accuracy curve\n",
        "!pip -qq install livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw83tjrgdqNO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error             \n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler              # scaling functions from sklearn\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split                  # search on hyperparameters\n",
        "from functools import partial                                                             # partial functions\n",
        "import tensorflow as tf                                                                   # importing tensorflow library\n",
        "from tensorflow import keras                                                              # importing keras package\n",
        "from scipy.stats import reciprocal \n",
        "from sklearn.neural_network import MLPRegressor                                           # importing MLP regressor            \n",
        "from tensorflow.keras.optimizers import SGD                                               # stochastic Gradient Descent\n",
        "from tensorflow.keras.utils import to_categorical                                         # converting a class to categorical data type\n",
        "from keras.datasets import mnist                                                          # load mnist dataset\n",
        "import livelossplot                                                                       # visualize loss and accuracy \n",
        "from keras.models import Sequential                                                       # using keras importing Sequential Model\n",
        "from keras.layers import Activation, Dense, Input, Flatten, Dropout, BatchNormalization   # using keras importing layers                                    \n",
        "from keras.callbacks import EarlyStopping                                                 # to stop the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLVYkKdrdqNV"
      },
      "source": [
        "### Building a Regression MLP "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5YpbtWMdqNX"
      },
      "source": [
        "Here, in this implementation, we will be using California housing problem and tackle it using a regression neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE9ZZHrjdqNY"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j2NZQqhkdqNZ",
        "outputId": "e75ee7ef-feda-40c9-cf3a-98b691baedd8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6630758a-f879-4a22-8b7f-95a44e052d10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6630758a-f879-4a22-8b7f-95a44e052d10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6630758a-f879-4a22-8b7f-95a44e052d10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6630758a-f879-4a22-8b7f-95a44e052d10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -114.31     34.19                15.0       5612.0          1283.0   \n",
              "1    -114.47     34.40                19.0       7650.0          1901.0   \n",
              "2    -114.56     33.69                17.0        720.0           174.0   \n",
              "3    -114.57     33.64                14.0       1501.0           337.0   \n",
              "4    -114.57     33.57                20.0       1454.0           326.0   \n",
              "\n",
              "   population  households  median_income  median_house_value  \n",
              "0      1015.0       472.0         1.4936             66900.0  \n",
              "1      1129.0       463.0         1.8200             80100.0  \n",
              "2       333.0       117.0         1.6509             85700.0  \n",
              "3       515.0       226.0         3.1917             73400.0  \n",
              "4       624.0       262.0         1.9250             65500.0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train dataset\n",
        "df_train = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-aToiAvNdqNb",
        "outputId": "3ae5f9da-d6a9-4364-80df-f8a8a1377e9f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-26f96fc3-2410-4a3e-a284-34c02fd0db5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26f96fc3-2410-4a3e-a284-34c02fd0db5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26f96fc3-2410-4a3e-a284-34c02fd0db5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26f96fc3-2410-4a3e-a284-34c02fd0db5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -122.05     37.37                27.0       3885.0           661.0   \n",
              "1    -118.30     34.26                43.0       1510.0           310.0   \n",
              "2    -117.81     33.78                27.0       3589.0           507.0   \n",
              "3    -118.36     33.82                28.0         67.0            15.0   \n",
              "4    -119.67     36.33                19.0       1241.0           244.0   \n",
              "\n",
              "   population  households  median_income  median_house_value  \n",
              "0      1537.0       606.0         6.6085            344700.0  \n",
              "1       809.0       277.0         3.5990            176500.0  \n",
              "2      1484.0       495.0         5.7934            270500.0  \n",
              "3        49.0        11.0         6.1359            330000.0  \n",
              "4       850.0       237.0         2.9375             81700.0  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test dataset\n",
        "df_test = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyyGRVgVdqNc",
        "outputId": "a44d2e52-67bd-40f1-a816-43e7b9dde2eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17000 entries, 0 to 16999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           17000 non-null  float64\n",
            " 1   latitude            17000 non-null  float64\n",
            " 2   housing_median_age  17000 non-null  float64\n",
            " 3   total_rooms         17000 non-null  float64\n",
            " 4   total_bedrooms      17000 non-null  float64\n",
            " 5   population          17000 non-null  float64\n",
            " 6   households          17000 non-null  float64\n",
            " 7   median_income       17000 non-null  float64\n",
            " 8   median_house_value  17000 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.2 MB\n"
          ]
        }
      ],
      "source": [
        "# printing train dataset information\n",
        "df_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV-lUMrIdqNd",
        "outputId": "86cadb4d-b551-495c-f9f6-873f8bd9a85f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           3000 non-null   float64\n",
            " 1   latitude            3000 non-null   float64\n",
            " 2   housing_median_age  3000 non-null   float64\n",
            " 3   total_rooms         3000 non-null   float64\n",
            " 4   total_bedrooms      3000 non-null   float64\n",
            " 5   population          3000 non-null   float64\n",
            " 6   households          3000 non-null   float64\n",
            " 7   median_income       3000 non-null   float64\n",
            " 8   median_house_value  3000 non-null   float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 211.1 KB\n"
          ]
        }
      ],
      "source": [
        "# printing test dataset information\n",
        "df_test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Vwj6P4dqNe"
      },
      "source": [
        "#### Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wISS_1e2dqNf"
      },
      "outputs": [],
      "source": [
        "X_train = df_train.drop('median_house_value',axis=1)\n",
        "y_train = df_train['median_house_value']\n",
        "X_test = df_test.drop('median_house_value',axis=1)\n",
        "y_test = df_test['median_house_value']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Zhd27edqNg"
      },
      "source": [
        "#### Scaling Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLnjSOd6dqNh"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train= scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDSuABdjdqNi"
      },
      "source": [
        "There are two ways to implement MLP regressor one is using keras and the other way is using Scikit-Learn. In this section, we will discuss both two ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fnY7eWpdqNj"
      },
      "source": [
        "#### 1. Using Keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzTaNnmudqNk"
      },
      "source": [
        "Building, training, evaluating, and using a regression MLP using the Sequential API to make  predictions  is  quite  similar  to  what  we  did  for classification.  The  main  differences  are  the  fact  that  the  output  layer  has  a  single  neuron  (since  we  only  want  to predict  a  single  value)  and  uses  no  activation  function,  and  the  loss  function  is  the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdUBydS0dqNl"
      },
      "outputs": [],
      "source": [
        "# create a model with two layers\n",
        "model = Sequential([\n",
        "                    Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "                    Dense(1)\n",
        "                    ])\n",
        "model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glssNoESfQC_"
      },
      "source": [
        "Keras supports the early stopping of training via a callback called EarlyStopping.\n",
        "\n",
        "This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n",
        "\n",
        "The EarlyStopping callback is configured when instantiated via arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7MdewIYdqNm",
        "outputId": "0778dd98-e6f2-4d49-8f7e-8d5b7bf9c767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "133/133 [==============================] - 2s 4ms/step - loss: 56424697856.0000 - val_loss: 55163424768.0000\n",
            "Epoch 2/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 56422809600.0000 - val_loss: 55160688640.0000\n",
            "Epoch 3/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56419192832.0000 - val_loss: 55156154368.0000\n",
            "Epoch 4/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 56413687808.0000 - val_loss: 55149842432.0000\n",
            "Epoch 5/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 56406470656.0000 - val_loss: 55141892096.0000\n",
            "Epoch 6/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56397660160.0000 - val_loss: 55132409856.0000\n",
            "Epoch 7/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56387358720.0000 - val_loss: 55121461248.0000\n",
            "Epoch 8/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56375595008.0000 - val_loss: 55109115904.0000\n",
            "Epoch 9/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 56362459136.0000 - val_loss: 55095443456.0000\n",
            "Epoch 10/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 56347951104.0000 - val_loss: 55080443904.0000\n",
            "Epoch 11/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56332222464.0000 - val_loss: 55064215552.0000\n",
            "Epoch 12/400\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 56315211776.0000 - val_loss: 55046828032.0000\n",
            "Epoch 13/400\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 56297029632.0000 - val_loss: 55028240384.0000\n",
            "Epoch 14/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56277700608.0000 - val_loss: 55008538624.0000\n",
            "Epoch 15/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56257269760.0000 - val_loss: 54987755520.0000\n",
            "Epoch 16/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 56235761664.0000 - val_loss: 54965940224.0000\n",
            "Epoch 17/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 56213204992.0000 - val_loss: 54943133696.0000\n",
            "Epoch 18/400\n",
            "133/133 [==============================] - 1s 7ms/step - loss: 56189661184.0000 - val_loss: 54919241728.0000\n",
            "Epoch 19/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 56165068800.0000 - val_loss: 54894514176.0000\n",
            "Epoch 20/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56139575296.0000 - val_loss: 54868754432.0000\n",
            "Epoch 21/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 56113090560.0000 - val_loss: 54842101760.0000\n",
            "Epoch 22/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 56085716992.0000 - val_loss: 54814502912.0000\n",
            "Epoch 23/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56057413632.0000 - val_loss: 54786043904.0000\n",
            "Epoch 24/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 56028241920.0000 - val_loss: 54756728832.0000\n",
            "Epoch 25/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 55998218240.0000 - val_loss: 54726541312.0000\n",
            "Epoch 26/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 55967334400.0000 - val_loss: 54695550976.0000\n",
            "Epoch 27/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 55935553536.0000 - val_loss: 54663700480.0000\n",
            "Epoch 28/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 55902982144.0000 - val_loss: 54631043072.0000\n",
            "Epoch 29/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 55869571072.0000 - val_loss: 54597562368.0000\n",
            "Epoch 30/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 55835344896.0000 - val_loss: 54563282944.0000\n",
            "Epoch 31/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 55800328192.0000 - val_loss: 54528237568.0000\n",
            "Epoch 32/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 55764508672.0000 - val_loss: 54492368896.0000\n",
            "Epoch 33/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 55727902720.0000 - val_loss: 54455693312.0000\n",
            "Epoch 34/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 55690518528.0000 - val_loss: 54418350080.0000\n",
            "Epoch 35/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 55652372480.0000 - val_loss: 54380265472.0000\n",
            "Epoch 36/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 55613456384.0000 - val_loss: 54341292032.0000\n",
            "Epoch 37/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 55573770240.0000 - val_loss: 54301614080.0000\n",
            "Epoch 38/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 55533305856.0000 - val_loss: 54261211136.0000\n",
            "Epoch 39/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 55492128768.0000 - val_loss: 54220103680.0000\n",
            "Epoch 40/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 55450214400.0000 - val_loss: 54178156544.0000\n",
            "Epoch 41/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 55407558656.0000 - val_loss: 54135603200.0000\n",
            "Epoch 42/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 55364194304.0000 - val_loss: 54092226560.0000\n",
            "Epoch 43/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55320047616.0000 - val_loss: 54048288768.0000\n",
            "Epoch 44/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55275180032.0000 - val_loss: 54003449856.0000\n",
            "Epoch 45/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55229607936.0000 - val_loss: 53958033408.0000\n",
            "Epoch 46/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55183294464.0000 - val_loss: 53911805952.0000\n",
            "Epoch 47/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55136272384.0000 - val_loss: 53864878080.0000\n",
            "Epoch 48/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55088541696.0000 - val_loss: 53817225216.0000\n",
            "Epoch 49/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55040098304.0000 - val_loss: 53768941568.0000\n",
            "Epoch 50/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54990938112.0000 - val_loss: 53720018944.0000\n",
            "Epoch 51/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54941126656.0000 - val_loss: 53670264832.0000\n",
            "Epoch 52/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54890553344.0000 - val_loss: 53619924992.0000\n",
            "Epoch 53/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54839304192.0000 - val_loss: 53568729088.0000\n",
            "Epoch 54/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54787358720.0000 - val_loss: 53516996608.0000\n",
            "Epoch 55/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54734721024.0000 - val_loss: 53464657920.0000\n",
            "Epoch 56/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54681407488.0000 - val_loss: 53411487744.0000\n",
            "Epoch 57/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54627364864.0000 - val_loss: 53357686784.0000\n",
            "Epoch 58/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54572670976.0000 - val_loss: 53303123968.0000\n",
            "Epoch 59/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54517284864.0000 - val_loss: 53248008192.0000\n",
            "Epoch 60/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54461214720.0000 - val_loss: 53192175616.0000\n",
            "Epoch 61/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54404460544.0000 - val_loss: 53135634432.0000\n",
            "Epoch 62/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54347075584.0000 - val_loss: 53078446080.0000\n",
            "Epoch 63/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54288982016.0000 - val_loss: 53020581888.0000\n",
            "Epoch 64/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54230253568.0000 - val_loss: 52962168832.0000\n",
            "Epoch 65/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54170820608.0000 - val_loss: 52903067648.0000\n",
            "Epoch 66/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54110736384.0000 - val_loss: 52843229184.0000\n",
            "Epoch 67/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54049980416.0000 - val_loss: 52782825472.0000\n",
            "Epoch 68/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53988564992.0000 - val_loss: 52721602560.0000\n",
            "Epoch 69/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53926494208.0000 - val_loss: 52659822592.0000\n",
            "Epoch 70/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53863776256.0000 - val_loss: 52597403648.0000\n",
            "Epoch 71/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53800382464.0000 - val_loss: 52534378496.0000\n",
            "Epoch 72/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53736370176.0000 - val_loss: 52470554624.0000\n",
            "Epoch 73/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53671682048.0000 - val_loss: 52406353920.0000\n",
            "Epoch 74/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53606363136.0000 - val_loss: 52341329920.0000\n",
            "Epoch 75/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53540413440.0000 - val_loss: 52275740672.0000\n",
            "Epoch 76/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53473824768.0000 - val_loss: 52209487872.0000\n",
            "Epoch 77/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53406572544.0000 - val_loss: 52142542848.0000\n",
            "Epoch 78/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53338664960.0000 - val_loss: 52075089920.0000\n",
            "Epoch 79/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53270163456.0000 - val_loss: 52006981632.0000\n",
            "Epoch 80/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53200990208.0000 - val_loss: 51938230272.0000\n",
            "Epoch 81/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 53131157504.0000 - val_loss: 51868848128.0000\n",
            "Epoch 82/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 53060739072.0000 - val_loss: 51798798336.0000\n",
            "Epoch 83/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 52989693952.0000 - val_loss: 51728125952.0000\n",
            "Epoch 84/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 52918022144.0000 - val_loss: 51656912896.0000\n",
            "Epoch 85/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 52845731840.0000 - val_loss: 51585060864.0000\n",
            "Epoch 86/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52772810752.0000 - val_loss: 51512594432.0000\n",
            "Epoch 87/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52699279360.0000 - val_loss: 51439566848.0000\n",
            "Epoch 88/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52625162240.0000 - val_loss: 51365728256.0000\n",
            "Epoch 89/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 52550410240.0000 - val_loss: 51291545600.0000\n",
            "Epoch 90/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52475060224.0000 - val_loss: 51216662528.0000\n",
            "Epoch 91/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52399116288.0000 - val_loss: 51141099520.0000\n",
            "Epoch 92/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52322570240.0000 - val_loss: 51065065472.0000\n",
            "Epoch 93/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52245413888.0000 - val_loss: 50988466176.0000\n",
            "Epoch 94/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52167680000.0000 - val_loss: 50911186944.0000\n",
            "Epoch 95/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52089327616.0000 - val_loss: 50833248256.0000\n",
            "Epoch 96/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52010364928.0000 - val_loss: 50755026944.0000\n",
            "Epoch 97/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51930873856.0000 - val_loss: 50675884032.0000\n",
            "Epoch 98/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51850719232.0000 - val_loss: 50596302848.0000\n",
            "Epoch 99/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51770007552.0000 - val_loss: 50516070400.0000\n",
            "Epoch 100/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51688730624.0000 - val_loss: 50435420160.0000\n",
            "Epoch 101/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51606863872.0000 - val_loss: 50354085888.0000\n",
            "Epoch 102/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51524411392.0000 - val_loss: 50272317440.0000\n",
            "Epoch 103/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51441373184.0000 - val_loss: 50189758464.0000\n",
            "Epoch 104/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51357794304.0000 - val_loss: 50106613760.0000\n",
            "Epoch 105/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51273646080.0000 - val_loss: 50023116800.0000\n",
            "Epoch 106/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51188920320.0000 - val_loss: 49939111936.0000\n",
            "Epoch 107/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51103633408.0000 - val_loss: 49854353408.0000\n",
            "Epoch 108/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51017793536.0000 - val_loss: 49769111552.0000\n",
            "Epoch 109/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50931363840.0000 - val_loss: 49683292160.0000\n",
            "Epoch 110/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50844405760.0000 - val_loss: 49596952576.0000\n",
            "Epoch 111/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50756890624.0000 - val_loss: 49509982208.0000\n",
            "Epoch 112/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50668785664.0000 - val_loss: 49422643200.0000\n",
            "Epoch 113/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50580156416.0000 - val_loss: 49334616064.0000\n",
            "Epoch 114/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50490974208.0000 - val_loss: 49246154752.0000\n",
            "Epoch 115/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50401255424.0000 - val_loss: 49157058560.0000\n",
            "Epoch 116/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50311004160.0000 - val_loss: 49067524096.0000\n",
            "Epoch 117/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50220191744.0000 - val_loss: 48977305600.0000\n",
            "Epoch 118/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50128855040.0000 - val_loss: 48886566912.0000\n",
            "Epoch 119/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50036985856.0000 - val_loss: 48795492352.0000\n",
            "Epoch 120/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49944604672.0000 - val_loss: 48703901696.0000\n",
            "Epoch 121/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49851699200.0000 - val_loss: 48611651584.0000\n",
            "Epoch 122/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49758253056.0000 - val_loss: 48518971392.0000\n",
            "Epoch 123/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49664286720.0000 - val_loss: 48425721856.0000\n",
            "Epoch 124/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49569857536.0000 - val_loss: 48331866112.0000\n",
            "Epoch 125/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49474859008.0000 - val_loss: 48237654016.0000\n",
            "Epoch 126/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 49379364864.0000 - val_loss: 48142819328.0000\n",
            "Epoch 127/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49283358720.0000 - val_loss: 48047603712.0000\n",
            "Epoch 128/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49186844672.0000 - val_loss: 47951974400.0000\n",
            "Epoch 129/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 49089765376.0000 - val_loss: 47855751168.0000\n",
            "Epoch 130/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48992247808.0000 - val_loss: 47758749696.0000\n",
            "Epoch 131/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48894193664.0000 - val_loss: 47661580288.0000\n",
            "Epoch 132/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48795672576.0000 - val_loss: 47563853824.0000\n",
            "Epoch 133/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48696623104.0000 - val_loss: 47465603072.0000\n",
            "Epoch 134/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48597114880.0000 - val_loss: 47366852608.0000\n",
            "Epoch 135/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48497164288.0000 - val_loss: 47267708928.0000\n",
            "Epoch 136/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48396701696.0000 - val_loss: 47167991808.0000\n",
            "Epoch 137/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48295718912.0000 - val_loss: 47067947008.0000\n",
            "Epoch 138/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48194260992.0000 - val_loss: 46967308288.0000\n",
            "Epoch 139/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48092356608.0000 - val_loss: 46866219008.0000\n",
            "Epoch 140/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47989952512.0000 - val_loss: 46764564480.0000\n",
            "Epoch 141/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47887073280.0000 - val_loss: 46662692864.0000\n",
            "Epoch 142/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47783792640.0000 - val_loss: 46560251904.0000\n",
            "Epoch 143/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 47680020480.0000 - val_loss: 46457389056.0000\n",
            "Epoch 144/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47575781376.0000 - val_loss: 46354026496.0000\n",
            "Epoch 145/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 47471050752.0000 - val_loss: 46250299392.0000\n",
            "Epoch 146/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47365869568.0000 - val_loss: 46145826816.0000\n",
            "Epoch 147/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47260225536.0000 - val_loss: 46041161728.0000\n",
            "Epoch 148/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47154216960.0000 - val_loss: 45935939584.0000\n",
            "Epoch 149/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47047770112.0000 - val_loss: 45830434816.0000\n",
            "Epoch 150/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46940835840.0000 - val_loss: 45724348416.0000\n",
            "Epoch 151/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46833446912.0000 - val_loss: 45617950720.0000\n",
            "Epoch 152/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46725632000.0000 - val_loss: 45511032832.0000\n",
            "Epoch 153/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 46617350144.0000 - val_loss: 45403885568.0000\n",
            "Epoch 154/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46508654592.0000 - val_loss: 45295968256.0000\n",
            "Epoch 155/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46399524864.0000 - val_loss: 45187883008.0000\n",
            "Epoch 156/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46290010112.0000 - val_loss: 45079347200.0000\n",
            "Epoch 157/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46180032512.0000 - val_loss: 44970287104.0000\n",
            "Epoch 158/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46069604352.0000 - val_loss: 44860932096.0000\n",
            "Epoch 159/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45958799360.0000 - val_loss: 44751040512.0000\n",
            "Epoch 160/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45847580672.0000 - val_loss: 44640628736.0000\n",
            "Epoch 161/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 45735989248.0000 - val_loss: 44530180096.0000\n",
            "Epoch 162/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45624000512.0000 - val_loss: 44419264512.0000\n",
            "Epoch 163/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45511581696.0000 - val_loss: 44307812352.0000\n",
            "Epoch 164/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45398806528.0000 - val_loss: 44195860480.0000\n",
            "Epoch 165/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 45285621760.0000 - val_loss: 44083838976.0000\n",
            "Epoch 166/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45172056064.0000 - val_loss: 43971354624.0000\n",
            "Epoch 167/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45058072576.0000 - val_loss: 43858497536.0000\n",
            "Epoch 168/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 44943736832.0000 - val_loss: 43745071104.0000\n",
            "Epoch 169/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44829028352.0000 - val_loss: 43631276032.0000\n",
            "Epoch 170/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 44713938944.0000 - val_loss: 43517255680.0000\n",
            "Epoch 171/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44598439936.0000 - val_loss: 43402936320.0000\n",
            "Epoch 172/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44482613248.0000 - val_loss: 43288219648.0000\n",
            "Epoch 173/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44366397440.0000 - val_loss: 43173113856.0000\n",
            "Epoch 174/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44249845760.0000 - val_loss: 43057664000.0000\n",
            "Epoch 175/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44132933632.0000 - val_loss: 42941804544.0000\n",
            "Epoch 176/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44015652864.0000 - val_loss: 42825465856.0000\n",
            "Epoch 177/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43897991168.0000 - val_loss: 42709118976.0000\n",
            "Epoch 178/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43780050944.0000 - val_loss: 42592186368.0000\n",
            "Epoch 179/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43661746176.0000 - val_loss: 42474971136.0000\n",
            "Epoch 180/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43543089152.0000 - val_loss: 42357473280.0000\n",
            "Epoch 181/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43424088064.0000 - val_loss: 42239643648.0000\n",
            "Epoch 182/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43304771584.0000 - val_loss: 42121416704.0000\n",
            "Epoch 183/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43185143808.0000 - val_loss: 42002833408.0000\n",
            "Epoch 184/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43065217024.0000 - val_loss: 41883885568.0000\n",
            "Epoch 185/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42944864256.0000 - val_loss: 41765011456.0000\n",
            "Epoch 186/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42824151040.0000 - val_loss: 41645608960.0000\n",
            "Epoch 187/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42703187968.0000 - val_loss: 41525608448.0000\n",
            "Epoch 188/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42581909504.0000 - val_loss: 41405644800.0000\n",
            "Epoch 189/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42460393472.0000 - val_loss: 41285042176.0000\n",
            "Epoch 190/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42338488320.0000 - val_loss: 41164644352.0000\n",
            "Epoch 191/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42216316928.0000 - val_loss: 41043566592.0000\n",
            "Epoch 192/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42093801472.0000 - val_loss: 40922218496.0000\n",
            "Epoch 193/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41970995200.0000 - val_loss: 40800739328.0000\n",
            "Epoch 194/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 41847980032.0000 - val_loss: 40678629376.0000\n",
            "Epoch 195/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41724588032.0000 - val_loss: 40556761088.0000\n",
            "Epoch 196/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 41600987136.0000 - val_loss: 40434274304.0000\n",
            "Epoch 197/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 41477058560.0000 - val_loss: 40311689216.0000\n",
            "Epoch 198/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 41352896512.0000 - val_loss: 40188624896.0000\n",
            "Epoch 199/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 41228427264.0000 - val_loss: 40065507328.0000\n",
            "Epoch 200/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 41103704064.0000 - val_loss: 39941959680.0000\n",
            "Epoch 201/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 40978694144.0000 - val_loss: 39818240000.0000\n",
            "Epoch 202/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 40853401600.0000 - val_loss: 39694299136.0000\n",
            "Epoch 203/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40727883776.0000 - val_loss: 39570100224.0000\n",
            "Epoch 204/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40602103808.0000 - val_loss: 39445573632.0000\n",
            "Epoch 205/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40476069888.0000 - val_loss: 39320743936.0000\n",
            "Epoch 206/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40349802496.0000 - val_loss: 39195766784.0000\n",
            "Epoch 207/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40223346688.0000 - val_loss: 39070490624.0000\n",
            "Epoch 208/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40096628736.0000 - val_loss: 38945345536.0000\n",
            "Epoch 209/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39969665024.0000 - val_loss: 38819614720.0000\n",
            "Epoch 210/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39842398208.0000 - val_loss: 38693736448.0000\n",
            "Epoch 211/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 39714975744.0000 - val_loss: 38567440384.0000\n",
            "Epoch 212/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39587299328.0000 - val_loss: 38441488384.0000\n",
            "Epoch 213/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39459463168.0000 - val_loss: 38314926080.0000\n",
            "Epoch 214/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39331401728.0000 - val_loss: 38188032000.0000\n",
            "Epoch 215/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39203078144.0000 - val_loss: 38061236224.0000\n",
            "Epoch 216/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 39074603008.0000 - val_loss: 37933875200.0000\n",
            "Epoch 217/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 38945873920.0000 - val_loss: 37806604288.0000\n",
            "Epoch 218/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 38817038336.0000 - val_loss: 37679198208.0000\n",
            "Epoch 219/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38687965184.0000 - val_loss: 37551529984.0000\n",
            "Epoch 220/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38558687232.0000 - val_loss: 37423587328.0000\n",
            "Epoch 221/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38429261824.0000 - val_loss: 37295493120.0000\n",
            "Epoch 222/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38299627520.0000 - val_loss: 37167300608.0000\n",
            "Epoch 223/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 38169817088.0000 - val_loss: 37038665728.0000\n",
            "Epoch 224/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38039830528.0000 - val_loss: 36910194688.0000\n",
            "Epoch 225/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37909667840.0000 - val_loss: 36781719552.0000\n",
            "Epoch 226/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 37779369984.0000 - val_loss: 36652503040.0000\n",
            "Epoch 227/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37648871424.0000 - val_loss: 36523315200.0000\n",
            "Epoch 228/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37518249984.0000 - val_loss: 36394278912.0000\n",
            "Epoch 229/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37387534336.0000 - val_loss: 36264980480.0000\n",
            "Epoch 230/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37256577024.0000 - val_loss: 36135727104.0000\n",
            "Epoch 231/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 37125513216.0000 - val_loss: 36005982208.0000\n",
            "Epoch 232/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36994273280.0000 - val_loss: 35876143104.0000\n",
            "Epoch 233/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36862971904.0000 - val_loss: 35746238464.0000\n",
            "Epoch 234/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36731498496.0000 - val_loss: 35616116736.0000\n",
            "Epoch 235/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36599881728.0000 - val_loss: 35486031872.0000\n",
            "Epoch 236/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36468137984.0000 - val_loss: 35356143616.0000\n",
            "Epoch 237/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36336300032.0000 - val_loss: 35225690112.0000\n",
            "Epoch 238/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 36204318720.0000 - val_loss: 35095199744.0000\n",
            "Epoch 239/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36072271872.0000 - val_loss: 34964512768.0000\n",
            "Epoch 240/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35940102144.0000 - val_loss: 34834034688.0000\n",
            "Epoch 241/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35807809536.0000 - val_loss: 34703245312.0000\n",
            "Epoch 242/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35675381760.0000 - val_loss: 34572296192.0000\n",
            "Epoch 243/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35542884352.0000 - val_loss: 34441203712.0000\n",
            "Epoch 244/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35410284544.0000 - val_loss: 34310238208.0000\n",
            "Epoch 245/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 35277668352.0000 - val_loss: 34178916352.0000\n",
            "Epoch 246/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 35144925184.0000 - val_loss: 34047969280.0000\n",
            "Epoch 247/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 35012132864.0000 - val_loss: 33916559360.0000\n",
            "Epoch 248/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 34879246336.0000 - val_loss: 33785284608.0000\n",
            "Epoch 249/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 34746265600.0000 - val_loss: 33653995520.0000\n",
            "Epoch 250/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 34613272576.0000 - val_loss: 33522395136.0000\n",
            "Epoch 251/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 34480185344.0000 - val_loss: 33390931968.0000\n",
            "Epoch 252/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 34347137024.0000 - val_loss: 33259233280.0000\n",
            "Epoch 253/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34214000640.0000 - val_loss: 33127829504.0000\n",
            "Epoch 254/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 34080829440.0000 - val_loss: 32996239360.0000\n",
            "Epoch 255/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33947582464.0000 - val_loss: 32864600064.0000\n",
            "Epoch 256/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33814304768.0000 - val_loss: 32732909568.0000\n",
            "Epoch 257/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 33680996352.0000 - val_loss: 32601344000.0000\n",
            "Epoch 258/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33547669504.0000 - val_loss: 32469551104.0000\n",
            "Epoch 259/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 33414332416.0000 - val_loss: 32337879040.0000\n",
            "Epoch 260/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 33280995328.0000 - val_loss: 32206176256.0000\n",
            "Epoch 261/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33147602944.0000 - val_loss: 32074469376.0000\n",
            "Epoch 262/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33014226944.0000 - val_loss: 31942516736.0000\n",
            "Epoch 263/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32880881664.0000 - val_loss: 31810664448.0000\n",
            "Epoch 264/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 32747479040.0000 - val_loss: 31679037440.0000\n",
            "Epoch 265/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32614129664.0000 - val_loss: 31547410432.0000\n",
            "Epoch 266/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 32480841728.0000 - val_loss: 31415599104.0000\n",
            "Epoch 267/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32347541504.0000 - val_loss: 31284051968.0000\n",
            "Epoch 268/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32214235136.0000 - val_loss: 31152535552.0000\n",
            "Epoch 269/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32080887808.0000 - val_loss: 31020974080.0000\n",
            "Epoch 270/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31947608064.0000 - val_loss: 30889138176.0000\n",
            "Epoch 271/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31814334464.0000 - val_loss: 30757619712.0000\n",
            "Epoch 272/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31681161216.0000 - val_loss: 30626033664.0000\n",
            "Epoch 273/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31548010496.0000 - val_loss: 30494541824.0000\n",
            "Epoch 274/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31414890496.0000 - val_loss: 30363357184.0000\n",
            "Epoch 275/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31281848320.0000 - val_loss: 30231910400.0000\n",
            "Epoch 276/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31148935168.0000 - val_loss: 30100635648.0000\n",
            "Epoch 277/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31016085504.0000 - val_loss: 29969381376.0000\n",
            "Epoch 278/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30883241984.0000 - val_loss: 29838530560.0000\n",
            "Epoch 279/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30750464000.0000 - val_loss: 29707366400.0000\n",
            "Epoch 280/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30617796608.0000 - val_loss: 29576259584.0000\n",
            "Epoch 281/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 30485198848.0000 - val_loss: 29445562368.0000\n",
            "Epoch 282/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30352668672.0000 - val_loss: 29314715648.0000\n",
            "Epoch 283/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30220222464.0000 - val_loss: 29183987712.0000\n",
            "Epoch 284/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30087872512.0000 - val_loss: 29053509632.0000\n",
            "Epoch 285/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29955618816.0000 - val_loss: 28923080704.0000\n",
            "Epoch 286/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29823514624.0000 - val_loss: 28792422400.0000\n",
            "Epoch 287/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29691521024.0000 - val_loss: 28662290432.0000\n",
            "Epoch 288/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29559646208.0000 - val_loss: 28532076544.0000\n",
            "Epoch 289/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 29427853312.0000 - val_loss: 28402120704.0000\n",
            "Epoch 290/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29296185344.0000 - val_loss: 28272220160.0000\n",
            "Epoch 291/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29164679168.0000 - val_loss: 28142544896.0000\n",
            "Epoch 292/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 29033334784.0000 - val_loss: 28012937216.0000\n",
            "Epoch 293/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28902141952.0000 - val_loss: 27883327488.0000\n",
            "Epoch 294/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 28771033088.0000 - val_loss: 27754186752.0000\n",
            "Epoch 295/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28640069632.0000 - val_loss: 27625254912.0000\n",
            "Epoch 296/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28509329408.0000 - val_loss: 27495999488.0000\n",
            "Epoch 297/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28378664960.0000 - val_loss: 27367290880.0000\n",
            "Epoch 298/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28248242176.0000 - val_loss: 27238649856.0000\n",
            "Epoch 299/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 28117964800.0000 - val_loss: 27110184960.0000\n",
            "Epoch 300/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27987871744.0000 - val_loss: 26981775360.0000\n",
            "Epoch 301/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27857922048.0000 - val_loss: 26854047744.0000\n",
            "Epoch 302/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27728189440.0000 - val_loss: 26725578752.0000\n",
            "Epoch 303/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27598608384.0000 - val_loss: 26598184960.0000\n",
            "Epoch 304/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27469305856.0000 - val_loss: 26470559744.0000\n",
            "Epoch 305/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27340177408.0000 - val_loss: 26343323648.0000\n",
            "Epoch 306/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27211307008.0000 - val_loss: 26216402944.0000\n",
            "Epoch 307/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27082647552.0000 - val_loss: 26089367552.0000\n",
            "Epoch 308/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 26954192896.0000 - val_loss: 25962942464.0000\n",
            "Epoch 309/400\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 26825977856.0000 - val_loss: 25836359680.0000\n",
            "Epoch 310/400\n",
            "133/133 [==============================] - 2s 12ms/step - loss: 26697932800.0000 - val_loss: 25710073856.0000\n",
            "Epoch 311/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 26570141696.0000 - val_loss: 25584513024.0000\n",
            "Epoch 312/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 26442616832.0000 - val_loss: 25458890752.0000\n",
            "Epoch 313/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26315313152.0000 - val_loss: 25333329920.0000\n",
            "Epoch 314/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 26188269568.0000 - val_loss: 25208197120.0000\n",
            "Epoch 315/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 26061463552.0000 - val_loss: 25083392000.0000\n",
            "Epoch 316/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 25934893056.0000 - val_loss: 24958828544.0000\n",
            "Epoch 317/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 25808543744.0000 - val_loss: 24834177024.0000\n",
            "Epoch 318/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 25682577408.0000 - val_loss: 24709771264.0000\n",
            "Epoch 319/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 25556842496.0000 - val_loss: 24586074112.0000\n",
            "Epoch 320/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 25431398400.0000 - val_loss: 24462790656.0000\n",
            "Epoch 321/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 25306269696.0000 - val_loss: 24339261440.0000\n",
            "Epoch 322/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 25181390848.0000 - val_loss: 24216446976.0000\n",
            "Epoch 323/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25056759808.0000 - val_loss: 24093806592.0000\n",
            "Epoch 324/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24932395008.0000 - val_loss: 23971543040.0000\n",
            "Epoch 325/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24808384512.0000 - val_loss: 23849172992.0000\n",
            "Epoch 326/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24684699648.0000 - val_loss: 23727253504.0000\n",
            "Epoch 327/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24561391616.0000 - val_loss: 23606011904.0000\n",
            "Epoch 328/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24438405120.0000 - val_loss: 23484895232.0000\n",
            "Epoch 329/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 24315738112.0000 - val_loss: 23364063232.0000\n",
            "Epoch 330/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 24193452032.0000 - val_loss: 23243759616.0000\n",
            "Epoch 331/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 24071456768.0000 - val_loss: 23123953664.0000\n",
            "Epoch 332/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 23949795328.0000 - val_loss: 23003987968.0000\n",
            "Epoch 333/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 23828490240.0000 - val_loss: 22884700160.0000\n",
            "Epoch 334/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 23707502592.0000 - val_loss: 22765791232.0000\n",
            "Epoch 335/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 23586879488.0000 - val_loss: 22647144448.0000\n",
            "Epoch 336/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23466612736.0000 - val_loss: 22528839680.0000\n",
            "Epoch 337/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23346726912.0000 - val_loss: 22410688512.0000\n",
            "Epoch 338/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 23227203584.0000 - val_loss: 22293331968.0000\n",
            "Epoch 339/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23108083712.0000 - val_loss: 22176192512.0000\n",
            "Epoch 340/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22989391872.0000 - val_loss: 22059106304.0000\n",
            "Epoch 341/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22871105536.0000 - val_loss: 21942921216.0000\n",
            "Epoch 342/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22753204224.0000 - val_loss: 21826918400.0000\n",
            "Epoch 343/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22635646976.0000 - val_loss: 21711495168.0000\n",
            "Epoch 344/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22518425600.0000 - val_loss: 21596827648.0000\n",
            "Epoch 345/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22401658880.0000 - val_loss: 21481699328.0000\n",
            "Epoch 346/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22285330432.0000 - val_loss: 21367025664.0000\n",
            "Epoch 347/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22169389056.0000 - val_loss: 21253099520.0000\n",
            "Epoch 348/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22053949440.0000 - val_loss: 21139668992.0000\n",
            "Epoch 349/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21938952192.0000 - val_loss: 21026756608.0000\n",
            "Epoch 350/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 21824471040.0000 - val_loss: 20914046976.0000\n",
            "Epoch 351/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21710397440.0000 - val_loss: 20802099200.0000\n",
            "Epoch 352/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 21596749824.0000 - val_loss: 20690688000.0000\n",
            "Epoch 353/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 21483563008.0000 - val_loss: 20579495936.0000\n",
            "Epoch 354/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 21370750976.0000 - val_loss: 20469067776.0000\n",
            "Epoch 355/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 21258471424.0000 - val_loss: 20358621184.0000\n",
            "Epoch 356/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 21146689536.0000 - val_loss: 20248743936.0000\n",
            "Epoch 357/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 21035347968.0000 - val_loss: 20139247616.0000\n",
            "Epoch 358/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20924499968.0000 - val_loss: 20030580736.0000\n",
            "Epoch 359/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20814141440.0000 - val_loss: 19922372608.0000\n",
            "Epoch 360/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20704331776.0000 - val_loss: 19814569984.0000\n",
            "Epoch 361/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20595015680.0000 - val_loss: 19706957824.0000\n",
            "Epoch 362/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20486217728.0000 - val_loss: 19600541696.0000\n",
            "Epoch 363/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20377991168.0000 - val_loss: 19494105088.0000\n",
            "Epoch 364/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20270143488.0000 - val_loss: 19388471296.0000\n",
            "Epoch 365/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20162863104.0000 - val_loss: 19282923520.0000\n",
            "Epoch 366/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20056068096.0000 - val_loss: 19178387456.0000\n",
            "Epoch 367/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 19949899776.0000 - val_loss: 19074152448.0000\n",
            "Epoch 368/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 19844229120.0000 - val_loss: 18970310656.0000\n",
            "Epoch 369/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 19739070464.0000 - val_loss: 18867320832.0000\n",
            "Epoch 370/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 19634462720.0000 - val_loss: 18764931072.0000\n",
            "Epoch 371/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 19530373120.0000 - val_loss: 18662819840.0000\n",
            "Epoch 372/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 19426859008.0000 - val_loss: 18561394688.0000\n",
            "Epoch 373/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 19323879424.0000 - val_loss: 18460641280.0000\n",
            "Epoch 374/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 19221403648.0000 - val_loss: 18360678400.0000\n",
            "Epoch 375/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 19119673344.0000 - val_loss: 18260312064.0000\n",
            "Epoch 376/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 19018407936.0000 - val_loss: 18161215488.0000\n",
            "Epoch 377/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 18917758976.0000 - val_loss: 18062880768.0000\n",
            "Epoch 378/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 18817658880.0000 - val_loss: 17964902400.0000\n",
            "Epoch 379/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 18718212096.0000 - val_loss: 17867335680.0000\n",
            "Epoch 380/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 18619377664.0000 - val_loss: 17770315776.0000\n",
            "Epoch 381/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 18521096192.0000 - val_loss: 17674391552.0000\n",
            "Epoch 382/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 18423521280.0000 - val_loss: 17578956800.0000\n",
            "Epoch 383/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 18326528000.0000 - val_loss: 17484060672.0000\n",
            "Epoch 384/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 18230112256.0000 - val_loss: 17389594624.0000\n",
            "Epoch 385/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 18134358016.0000 - val_loss: 17296037888.0000\n",
            "Epoch 386/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 18039304192.0000 - val_loss: 17202825216.0000\n",
            "Epoch 387/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 17944879104.0000 - val_loss: 17110554624.0000\n",
            "Epoch 388/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 17851035648.0000 - val_loss: 17019193344.0000\n",
            "Epoch 389/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17757853696.0000 - val_loss: 16927903744.0000\n",
            "Epoch 390/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 17665284096.0000 - val_loss: 16837130240.0000\n",
            "Epoch 391/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 17573406720.0000 - val_loss: 16747512832.0000\n",
            "Epoch 392/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 17482233856.0000 - val_loss: 16658300928.0000\n",
            "Epoch 393/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 17391745024.0000 - val_loss: 16569715712.0000\n",
            "Epoch 394/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17301854208.0000 - val_loss: 16482290688.0000\n",
            "Epoch 395/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17212692480.0000 - val_loss: 16395255808.0000\n",
            "Epoch 396/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 17124230144.0000 - val_loss: 16308590592.0000\n",
            "Epoch 397/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 17036396544.0000 - val_loss: 16222908416.0000\n",
            "Epoch 398/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 16949314560.0000 - val_loss: 16138061824.0000\n",
            "Epoch 399/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 16862904320.0000 - val_loss: 16053803008.0000\n",
            "Epoch 400/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 16777175040.0000 - val_loss: 15970195456.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0403de6e50>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# defining early stop \n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "# fitting the model\n",
        "model.fit(x=X_train,y=y_train.values,\n",
        "          validation_data=(X_test,y_test.values),\n",
        "          batch_size=128,epochs=400, callbacks=[early_stop])\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzYPgOOEdqNn",
        "outputId": "71b02af1-b549-40d8-c2ee-6d166ef9f795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 30)                270       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 31        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301\n",
            "Trainable params: 301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Sequential Model Summary \n",
        "model.summary() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0IzyLyTdqNo"
      },
      "source": [
        "##### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_0If9N3dqNp",
        "outputId": "42aea1f0-d239-47f6-e55a-b9b9c47baf63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[151323.72],\n",
              "       [149246.77],\n",
              "       [148054.86],\n",
              "       ...,\n",
              "       [113961.9 ],\n",
              "       [142815.25],\n",
              "       [159410.56]], dtype=float32)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osGmwIw7dqNq"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eou7_DeldqNr",
        "outputId": "4b802da1-ab3b-4950-9aae-f36d3d3b459a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "126373.24078630931"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sqrt(mean_squared_error(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52QacZPvdqNs"
      },
      "source": [
        "#### 2. Using Sci-kit Learn API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgmVeddndqNs"
      },
      "source": [
        "Using the same dataset, we will implement MLP regressor using sci-kit learn API. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVZ9KE8dqNt"
      },
      "source": [
        "In the below code, one hidden layer is modeled with 32 neurons. Considering\n",
        "the input and output layer, we have a total of 5 layers in the model. In case any optimizer is not mentioned then “Adam” is the default optimizer and it can manage a pretty large dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkBzCoekdqNu",
        "outputId": "fbd68825-2951-4f25-f733-448e55a0e583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ],
      "source": [
        "# implementing MLPregressor\n",
        "regr = MLPRegressor(hidden_layer_sizes=(32), activation=\"relu\", random_state=1, max_iter=500).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rruum2FKdqNv"
      },
      "source": [
        "##### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yPBpSl0dqN1",
        "outputId": "9c66fdb1-e5c1-4174-a12c-13cd1fb7fa3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([106943.7296704 , 105433.71377299, 104575.22478819, ...,\n",
              "        80490.94502437, 100875.35533722, 112639.58639599])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = regr.predict(X_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-HUagWGdqN2"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKmNxwzodqN3",
        "outputId": "b3d3ca88-439b-49a9-f9ab-9a6ebe8db9f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "152367.3177159458"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sqrt(mean_squared_error(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehdUQLWSdqN4"
      },
      "source": [
        "Now, let us look at the tuning of the neural network hyperparameters or hyperparameter regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYgkmzLEdqN5"
      },
      "source": [
        "### Fine-Tuning Neural Network Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEYREMR3dqN6"
      },
      "source": [
        "The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters  to  tweak.  Not  only  can  we  use  any  imaginable  network  architecture, but even in a simple MLP we can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization  logic,  and  much  more. \n",
        "\n",
        "One  option  is  to  simply  try  many  combinations  of  hyperparameters  and  see  which one works best on the validation set (or using K-fold cross-validation). For this, one approach  is  simply to use  GridSearchCV  or  RandomizedSearchCV  to  explore  the  hyperparameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WawLE5TndqN6"
      },
      "source": [
        "The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYsW5K_VdqN7"
      },
      "outputs": [],
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = Sequential()\n",
        "    options = {\"input_shape\": input_shape}\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(Dense(n_neurons, activation=\"relu\", **options))\n",
        "        options = {}\n",
        "    model.add(Dense(1, **options))\n",
        "    optimizer = SGD(learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBH4CYF5dqN8"
      },
      "source": [
        "This function creates a simple Sequential model for univariate regression (only one output  neuron),  with  the  given  input  shape  and  the  given  number  of  hidden  layers and  neurons,  and  it  compiles  it  using  an  SGD  optimizer  configured  with  the  given learning rate.\n",
        "\n",
        "Next, let’s create a KerasRegressor based on this build_model() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLNf-9oFdqN9",
        "outputId": "69220d19-83bd-49f8-9983-d08e91b58b53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUo6pP6JdqN-"
      },
      "source": [
        "We want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search. \n",
        "\n",
        "Let’s try to explore the number of hidden layers, the number of neurons, and the learning rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKMjD2XHdqN_"
      },
      "outputs": [],
      "source": [
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl4MtAGwdqOA",
        "outputId": "8c92990a-1df8-41cc-df9a-4bd2a5f16d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 35687347118931968.0000 - val_loss: 66503249920.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 20006658048.0000 - val_loss: 12863083520.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15153284096.0000 - val_loss: 12834814976.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147398144.0000 - val_loss: 12795695104.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152366592.0000 - val_loss: 12815329280.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15149869056.0000 - val_loss: 12831555584.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150866432.0000 - val_loss: 12803919872.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150437376.0000 - val_loss: 12831808512.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151600640.0000 - val_loss: 12807264256.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15152178176.0000 - val_loss: 12818038784.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148919808.0000 - val_loss: 12811130880.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15149544448.0000 - val_loss: 12807334912.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148908544.0000 - val_loss: 12814507008.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151098880.0000 - val_loss: 12837042176.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 10143247360.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22351510172073984.0000 - val_loss: 32860618752.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14190465024.0000 - val_loss: 12794771456.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12383262720.0000 - val_loss: 12825137152.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384935936.0000 - val_loss: 12803098624.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12383781888.0000 - val_loss: 12791874560.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384136192.0000 - val_loss: 12804720640.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384958464.0000 - val_loss: 12793538560.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385030144.0000 - val_loss: 12802010112.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384558080.0000 - val_loss: 12812804096.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381981696.0000 - val_loss: 12812497920.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386046976.0000 - val_loss: 12793123840.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384365568.0000 - val_loss: 12795220992.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385025024.0000 - val_loss: 12801051648.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386088960.0000 - val_loss: 12796500992.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385289216.0000 - val_loss: 12804208640.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 15605126144.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 95955848634826752.0000 - val_loss: 210071027712.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 29638723584.0000 - val_loss: 12841333760.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783982592.0000 - val_loss: 12796109824.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785475584.0000 - val_loss: 12802530304.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786028544.0000 - val_loss: 12798820352.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784517120.0000 - val_loss: 12803986432.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786683904.0000 - val_loss: 12818007040.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785503232.0000 - val_loss: 12841084928.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786557952.0000 - val_loss: 12830483456.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788139008.0000 - val_loss: 12806461440.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785452032.0000 - val_loss: 12804008960.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786368512.0000 - val_loss: 12815065088.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786315264.0000 - val_loss: 12811957248.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 14895932416.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 3ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 3ms/step - loss: 26162720269139968.0000 - val_loss: 13863612416.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12468436992.0000 - val_loss: 12826897408.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388247552.0000 - val_loss: 12793178112.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12383784960.0000 - val_loss: 12793967616.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: 12384559104.0000 - val_loss: 12829744128.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: 12386027520.0000 - val_loss: 12804640768.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386974720.0000 - val_loss: 12798807040.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12387312640.0000 - val_loss: 12808632320.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385909760.0000 - val_loss: 12807629824.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382895104.0000 - val_loss: 12794260480.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384173056.0000 - val_loss: 12794994688.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386866176.0000 - val_loss: 12792298496.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382396416.0000 - val_loss: 12791820288.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385766400.0000 - val_loss: 12820858880.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386095104.0000 - val_loss: 12829534208.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386487296.0000 - val_loss: 12804269056.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12383165440.0000 - val_loss: 12792811520.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12389276672.0000 - val_loss: 12804157440.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384730112.0000 - val_loss: 12794261504.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384909312.0000 - val_loss: 12812259328.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385627136.0000 - val_loss: 12812778496.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385113088.0000 - val_loss: 12792461312.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386193408.0000 - val_loss: 12791862272.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 15582321664.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 3816052741411700736.0000 - val_loss: 782547222528.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 64991690752.0000 - val_loss: 12798352384.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788290560.0000 - val_loss: 12830679040.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787490816.0000 - val_loss: 12811579392.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787101696.0000 - val_loss: 12839751680.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785231872.0000 - val_loss: 12805787648.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782448640.0000 - val_loss: 12792738816.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789312512.0000 - val_loss: 12800621568.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786056192.0000 - val_loss: 12816480256.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788013056.0000 - val_loss: 12804487168.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786787328.0000 - val_loss: 12798380032.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12787500032.0000 - val_loss: 12801822720.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786640896.0000 - val_loss: 12797555712.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785763328.0000 - val_loss: 12797186048.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12788088832.0000 - val_loss: 12811648000.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12788925440.0000 - val_loss: 12805336064.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787270656.0000 - val_loss: 12802423808.0000\n",
            "178/178 [==============================] - 1s 3ms/step - loss: 14856297472.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 2s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 723256893325180928.0000 - val_loss: 135208132608.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 21170176000.0000 - val_loss: 12798926848.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386320384.0000 - val_loss: 12798552064.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386083840.0000 - val_loss: 12795539456.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384520192.0000 - val_loss: 12801808384.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385907712.0000 - val_loss: 12791827456.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12387103744.0000 - val_loss: 12796752896.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386847744.0000 - val_loss: 12796659712.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386900992.0000 - val_loss: 12804914176.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385067008.0000 - val_loss: 12802622464.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384683008.0000 - val_loss: 12806228992.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385694720.0000 - val_loss: 12793242624.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385798144.0000 - val_loss: 12791904256.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386172928.0000 - val_loss: 12823028736.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384399360.0000 - val_loss: 12806841344.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12386985984.0000 - val_loss: 12805331968.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 15606740992.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 4ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 50463715328.0000 - val_loss: 38414110720.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 36814352384.0000 - val_loss: 28227289088.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 28442318848.0000 - val_loss: 22031298560.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 23297292288.0000 - val_loss: 18292860928.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 20149602304.0000 - val_loss: 16027163648.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18212229120.0000 - val_loss: 14678950912.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17028773888.0000 - val_loss: 13870906368.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16299701248.0000 - val_loss: 13393495040.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15851397120.0000 - val_loss: 13120036864.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15580270592.0000 - val_loss: 12962412544.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15413210112.0000 - val_loss: 12874270720.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15310344192.0000 - val_loss: 12827811840.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15247121408.0000 - val_loss: 12804403200.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15207975936.0000 - val_loss: 12794269696.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15183392768.0000 - val_loss: 12791804928.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15168662528.0000 - val_loss: 12792955904.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15160101888.0000 - val_loss: 12795803648.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15154466816.0000 - val_loss: 12799193088.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151051776.0000 - val_loss: 12802665472.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148940288.0000 - val_loss: 12806063104.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147604992.0000 - val_loss: 12809147392.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146836992.0000 - val_loss: 12811307008.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146390528.0000 - val_loss: 12813296640.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146092544.0000 - val_loss: 12814813184.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15145950208.0000 - val_loss: 12815737856.0000\n",
            "178/178 [==============================] - 1s 3ms/step - loss: 10098921472.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 155142356992.0000 - val_loss: 39723216896.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 34497200128.0000 - val_loss: 29165086720.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 25965058048.0000 - val_loss: 22719678464.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 20721225728.0000 - val_loss: 18778578944.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17494605824.0000 - val_loss: 16388732928.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15521482752.0000 - val_loss: 14940803072.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14309537792.0000 - val_loss: 14065464320.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13564890112.0000 - val_loss: 13537717248.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13107316736.0000 - val_loss: 13223280640.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12827836416.0000 - val_loss: 13036237824.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12655261696.0000 - val_loss: 12926828544.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12549890048.0000 - val_loss: 12862832640.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12483998720.0000 - val_loss: 12827596800.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12444478464.0000 - val_loss: 12807861248.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12419601408.0000 - val_loss: 12797932544.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12404458496.0000 - val_loss: 12793392128.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12395226112.0000 - val_loss: 12791908352.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12389535744.0000 - val_loss: 12792004608.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385910784.0000 - val_loss: 12792808448.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12383954944.0000 - val_loss: 12793738240.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382889984.0000 - val_loss: 12794816512.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382139392.0000 - val_loss: 12795712512.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381704192.0000 - val_loss: 12796535808.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381423616.0000 - val_loss: 12797254656.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381234176.0000 - val_loss: 12797976576.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12381084672.0000 - val_loss: 12798593024.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12380989440.0000 - val_loss: 12798994432.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 15597216768.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 19357654581248.0000 - val_loss: 55518068736.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 45349191680.0000 - val_loss: 39298342912.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 32774672384.0000 - val_loss: 29265930240.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 25042636800.0000 - val_loss: 23077060608.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 20311140352.0000 - val_loss: 19250423808.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17409382400.0000 - val_loss: 16866956288.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15625856000.0000 - val_loss: 15379738624.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14527350784.0000 - val_loss: 14451266560.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13854295040.0000 - val_loss: 13860587520.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13437493248.0000 - val_loss: 13492146176.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13183685632.0000 - val_loss: 13255929856.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13027660800.0000 - val_loss: 13106026496.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12932674560.0000 - val_loss: 13007810560.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12873804800.0000 - val_loss: 12945007616.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12838922240.0000 - val_loss: 12903984128.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12817332224.0000 - val_loss: 12875667456.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12803930112.0000 - val_loss: 12855954432.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12795438080.0000 - val_loss: 12842414080.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12790342656.0000 - val_loss: 12833338368.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787261440.0000 - val_loss: 12826553344.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785295360.0000 - val_loss: 12822012928.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784189440.0000 - val_loss: 12818524160.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783497216.0000 - val_loss: 12816316416.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783062016.0000 - val_loss: 12814138368.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782799872.0000 - val_loss: 12813035520.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782620672.0000 - val_loss: 12811731968.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782500864.0000 - val_loss: 12811041792.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782444544.0000 - val_loss: 12810209280.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782398464.0000 - val_loss: 12809824256.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782304256.0000 - val_loss: 12808940544.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782290944.0000 - val_loss: 12808287232.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782333952.0000 - val_loss: 12807725056.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782298112.0000 - val_loss: 12807433216.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782244864.0000 - val_loss: 12807089152.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782314496.0000 - val_loss: 12807449600.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782309376.0000 - val_loss: 12807564288.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782267392.0000 - val_loss: 12807817216.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782277632.0000 - val_loss: 12807586816.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782272512.0000 - val_loss: 12807139328.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782290944.0000 - val_loss: 12807059456.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782275584.0000 - val_loss: 12806784000.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: 12782277632.0000 - val_loss: 12806585344.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: 12782261248.0000 - val_loss: 12807134208.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782251008.0000 - val_loss: 12807262208.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782311424.0000 - val_loss: 12807754752.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782298112.0000 - val_loss: 12807681024.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782278656.0000 - val_loss: 12807901184.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782235648.0000 - val_loss: 12807793664.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782274560.0000 - val_loss: 12807524352.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782281728.0000 - val_loss: 12807203840.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782275584.0000 - val_loss: 12807291904.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782288896.0000 - val_loss: 12807227392.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 14877461504.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 37667731406848.0000 - val_loss: 61896290304.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 56652267520.0000 - val_loss: 43714830336.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 41604648960.0000 - val_loss: 32211771392.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 32015593472.0000 - val_loss: 24928753664.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 25898721280.0000 - val_loss: 20351277056.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22010169344.0000 - val_loss: 17459255296.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 19516997632.0000 - val_loss: 15652493312.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17933635584.0000 - val_loss: 14524294144.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16923426816.0000 - val_loss: 13825779712.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16278014976.0000 - val_loss: 13397819392.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15868565504.0000 - val_loss: 13138283520.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15607385088.0000 - val_loss: 12981226496.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15439377408.0000 - val_loss: 12890097664.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15332867072.0000 - val_loss: 12837626880.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15263354880.0000 - val_loss: 12810296320.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15220422656.0000 - val_loss: 12797364224.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15193257984.0000 - val_loss: 12792423424.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15175630848.0000 - val_loss: 12791994368.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15164841984.0000 - val_loss: 12793826304.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15157970944.0000 - val_loss: 12796511232.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15153611776.0000 - val_loss: 12799501312.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15150924800.0000 - val_loss: 12802700288.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148999680.0000 - val_loss: 12805126144.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147965440.0000 - val_loss: 12807702528.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147039744.0000 - val_loss: 12810013696.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146588160.0000 - val_loss: 12812210176.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146182656.0000 - val_loss: 12813580288.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146039296.0000 - val_loss: 12815183872.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 10097664000.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 97034838908862464.0000 - val_loss: 10673287331840.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8622401650688.0000 - val_loss: 6807099539456.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5502383161344.0000 - val_loss: 4342535946240.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3512972083200.0000 - val_loss: 2771593199616.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 2244472733696.0000 - val_loss: 1770188439552.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1435547992064.0000 - val_loss: 1132032557056.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 919770890240.0000 - val_loss: 725485944832.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 591019507712.0000 - val_loss: 466398740480.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 381305520128.0000 - val_loss: 301361397760.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 247617060864.0000 - val_loss: 196297850880.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 162378924032.0000 - val_loss: 129390247936.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 108041543680.0000 - val_loss: 86836109312.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 73382207488.0000 - val_loss: 59740893184.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 51282448384.0000 - val_loss: 42533146624.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 37193859072.0000 - val_loss: 31591385088.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 28202489856.0000 - val_loss: 24648806400.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22472200192.0000 - val_loss: 20239224832.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18806919168.0000 - val_loss: 17458860032.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16478336000.0000 - val_loss: 15697912832.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14989281280.0000 - val_loss: 14594311168.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14045724672.0000 - val_loss: 13903284224.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13443688448.0000 - val_loss: 13469820928.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13060406272.0000 - val_loss: 13201955840.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12816155648.0000 - val_loss: 13033776128.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12657727488.0000 - val_loss: 12930802688.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12556857344.0000 - val_loss: 12870065152.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12493357056.0000 - val_loss: 12833159168.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12452284416.0000 - val_loss: 12812136448.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12426094592.0000 - val_loss: 12800885760.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12409865216.0000 - val_loss: 12794919936.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: 12399079424.0000 - val_loss: 12792485888.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12392662016.0000 - val_loss: 12791808000.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388472832.0000 - val_loss: 12792095744.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12385656832.0000 - val_loss: 12792808448.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384032768.0000 - val_loss: 12793758720.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382911488.0000 - val_loss: 12794648576.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12382199808.0000 - val_loss: 12795710464.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381643776.0000 - val_loss: 12796615680.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381362176.0000 - val_loss: 12797506560.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381128704.0000 - val_loss: 12798057472.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381026304.0000 - val_loss: 12798363648.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12380997632.0000 - val_loss: 12798622720.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 15596625920.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 54476291440640000.0000 - val_loss: 6678591832064.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5370190757888.0000 - val_loss: 4266386784256.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3428814159872.0000 - val_loss: 2727740178432.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2191051194368.0000 - val_loss: 1746035802112.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1401738887168.0000 - val_loss: 1119682822144.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 898473721856.0000 - val_loss: 719944613888.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 577537114112.0000 - val_loss: 464809558016.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 372936343552.0000 - val_loss: 301900496896.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 242459262976.0000 - val_loss: 197851791360.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 159253626880.0000 - val_loss: 131333447680.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 106172809216.0000 - val_loss: 88819933184.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 72332050432.0000 - val_loss: 61617909760.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 50754502656.0000 - val_loss: 44210102272.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 36995162112.0000 - val_loss: 33042151424.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 28217145344.0000 - val_loss: 25884907520.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22623178752.0000 - val_loss: 21279674368.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 19052306432.0000 - val_loss: 18312368128.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16774894592.0000 - val_loss: 16405553152.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15325172736.0000 - val_loss: 15167474688.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14400591872.0000 - val_loss: 14371739648.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13816227840.0000 - val_loss: 13848670208.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13440240640.0000 - val_loss: 13505735680.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13200963584.0000 - val_loss: 13281484800.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 13049241600.0000 - val_loss: 13132573696.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12952453120.0000 - val_loss: 13033003008.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12890954752.0000 - val_loss: 12966544384.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12851625984.0000 - val_loss: 12920251392.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12826375168.0000 - val_loss: 12889444352.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12810723328.0000 - val_loss: 12866996224.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12800324608.0000 - val_loss: 12851170304.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12793749504.0000 - val_loss: 12840332288.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12789666816.0000 - val_loss: 12832397312.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787036160.0000 - val_loss: 12826174464.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785255424.0000 - val_loss: 12821726208.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784135168.0000 - val_loss: 12818320384.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783434752.0000 - val_loss: 12815939584.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783019008.0000 - val_loss: 12814345216.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782789632.0000 - val_loss: 12812955648.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782606336.0000 - val_loss: 12811553792.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782453760.0000 - val_loss: 12810694656.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782387200.0000 - val_loss: 12810001408.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782308352.0000 - val_loss: 12809717760.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782327808.0000 - val_loss: 12809388032.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782309376.0000 - val_loss: 12809060352.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782302208.0000 - val_loss: 12808781824.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782282752.0000 - val_loss: 12808561664.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782304256.0000 - val_loss: 12808333312.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782269440.0000 - val_loss: 12808275968.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782269440.0000 - val_loss: 12808417280.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782254080.0000 - val_loss: 12807746560.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782271488.0000 - val_loss: 12807909376.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782294016.0000 - val_loss: 12807885824.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782271488.0000 - val_loss: 12807720960.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782284800.0000 - val_loss: 12807378944.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782267392.0000 - val_loss: 12807505920.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782268416.0000 - val_loss: 12806833152.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782255104.0000 - val_loss: 12807336960.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782237696.0000 - val_loss: 12807400448.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782254080.0000 - val_loss: 12807290880.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782252032.0000 - val_loss: 12807561216.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782291968.0000 - val_loss: 12807552000.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782229504.0000 - val_loss: 12807677952.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782239744.0000 - val_loss: 12807530496.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782275584.0000 - val_loss: 12807420928.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782251008.0000 - val_loss: 12807308288.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782262272.0000 - val_loss: 12807437312.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 14878324736.0000\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [-1.35481020e+10             nan             nan             nan\n",
            "             nan             nan             nan             nan\n",
            " -1.35245332e+10 -1.35242049e+10]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "532/532 [==============================] - 2s 2ms/step - loss: 1662473400947310592.0000 - val_loss: 419732456996864.0000\n",
            "Epoch 2/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 305677083869184.0000 - val_loss: 213841673715712.0000\n",
            "Epoch 3/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 155744875839488.0000 - val_loss: 108948078198784.0000\n",
            "Epoch 4/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 79357389307904.0000 - val_loss: 55508094418944.0000\n",
            "Epoch 5/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 40438400548864.0000 - val_loss: 28282634371072.0000\n",
            "Epoch 6/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 20609494941696.0000 - val_loss: 14412680265728.0000\n",
            "Epoch 7/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 10506710548480.0000 - val_loss: 7346912231424.0000\n",
            "Epoch 8/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5359679832064.0000 - val_loss: 3747874340864.0000\n",
            "Epoch 9/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 2737242374144.0000 - val_loss: 1914575781888.0000\n",
            "Epoch 10/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 1401135955968.0000 - val_loss: 980889305088.0000\n",
            "Epoch 11/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 720451272704.0000 - val_loss: 505406259200.0000\n",
            "Epoch 12/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 373619654656.0000 - val_loss: 263338246144.0000\n",
            "Epoch 13/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 196940087296.0000 - val_loss: 140134531072.0000\n",
            "Epoch 14/100\n",
            "532/532 [==============================] - 2s 3ms/step - loss: 106931838976.0000 - val_loss: 77463502848.0000\n",
            "Epoch 15/100\n",
            "532/532 [==============================] - 2s 4ms/step - loss: 61084401664.0000 - val_loss: 45596061696.0000\n",
            "Epoch 16/100\n",
            "532/532 [==============================] - 2s 4ms/step - loss: 37724438528.0000 - val_loss: 29400707072.0000\n",
            "Epoch 17/100\n",
            "532/532 [==============================] - 2s 4ms/step - loss: 25820821504.0000 - val_loss: 21180448768.0000\n",
            "Epoch 18/100\n",
            "532/532 [==============================] - 1s 3ms/step - loss: 19754117120.0000 - val_loss: 17009314816.0000\n",
            "Epoch 19/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 16660177920.0000 - val_loss: 14895954944.0000\n",
            "Epoch 20/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 15083714560.0000 - val_loss: 13838668800.0000\n",
            "Epoch 21/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 14284002304.0000 - val_loss: 13307102208.0000\n",
            "Epoch 22/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13875362816.0000 - val_loss: 13039456256.0000\n",
            "Epoch 23/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13667134464.0000 - val_loss: 12909844480.0000\n",
            "Epoch 24/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13561957376.0000 - val_loss: 12845728768.0000\n",
            "Epoch 25/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13508143104.0000 - val_loss: 12815239168.0000\n",
            "Epoch 26/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13480288256.0000 - val_loss: 12800874496.0000\n",
            "Epoch 27/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13466079232.0000 - val_loss: 12794633216.0000\n",
            "Epoch 28/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13458837504.0000 - val_loss: 12792341504.0000\n",
            "Epoch 29/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13455158272.0000 - val_loss: 12791803904.0000\n",
            "Epoch 30/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13453426688.0000 - val_loss: 12791921664.0000\n",
            "Epoch 31/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13452591104.0000 - val_loss: 12792245248.0000\n",
            "Epoch 32/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13452144640.0000 - val_loss: 12792630272.0000\n",
            "Epoch 33/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13451859968.0000 - val_loss: 12793048064.0000\n",
            "Epoch 34/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13451816960.0000 - val_loss: 12793240576.0000\n",
            "Epoch 35/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13451751424.0000 - val_loss: 12793240576.0000\n",
            "Epoch 36/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13451739136.0000 - val_loss: 12793485312.0000\n",
            "Epoch 37/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13451747328.0000 - val_loss: 12793584640.0000\n",
            "Epoch 38/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13451687936.0000 - val_loss: 12793549824.0000\n",
            "Epoch 39/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 13451693056.0000 - val_loss: 12793603072.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3,\n",
              "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7f03fbb83b90>,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f03fbb2cdd0>,\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
              "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_test,y_test.values),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDyLtOT8dqOB"
      },
      "source": [
        "The above  exploration  may  last  many  hours  depending  on  the  hardware,  the  size  of  the dataset, the complexity of the model and the value of n_iter and cv. When it is over, we can access the best parameters found, the best score, and the trained Keras model like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgsAM812dqOC",
        "outputId": "4e639fe5-f91c-47b6-83f7-6a33f285753a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'learning_rate': 0.0003167910552381549, 'n_hidden': 1, 'n_neurons': 46}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# finding the best parameters\n",
        "rnd_search_cv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeOHn8WVdqOC",
        "outputId": "f0b4c8b9-8146-4928-b9ba-3abb0086dfd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-13524204885.333334"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# best score\n",
        "rnd_search_cv.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNY1vgDAdqOD"
      },
      "outputs": [],
      "source": [
        "# applying best parameters to the model for predictions\n",
        "model = rnd_search_cv.best_estimator_.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqut1OR3dqOE"
      },
      "source": [
        "Refer to the guidelines below for choosing the  number  of  hidden  layers  and  neurons  in  an  MLP,  and  selecting  appropriate  values  for some of the main hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9t_szGwdqOF"
      },
      "source": [
        "#### Number of Hidden Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqiYlS6AdqOG"
      },
      "source": [
        "- For simple problems, we can start with just one or two hidden layers and get the accurate results.\n",
        "- For more complex problems, we can gradually rampup the number of hidden layers, until we start overfitting the training set. Very complex  tasks,  such  as  large  image  classification  or  speech  recognition,  typically  require networks  with  dozens  of  layers  (or  even  hundreds,  but  not  fully  connected  ones),  and  they  need  a  huge  amount  of  training  data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZu_Fc1ydqOH"
      },
      "source": [
        "#### Number of Neurons per Hidden Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-1AYlmBdqOI"
      },
      "source": [
        "- We can try increasing the number of neurons gradually  until  the  network  starts  overfitting. \n",
        "- In general, it may be more advantageous to increase  the  number  of  layers  than  the  number  of  neurons  per  layer.\n",
        "- A  simpler  approach  is  to  pick  a  model  with  more  layers  and  neurons  than  we actually need, then use early stopping to prevent it from overfitting (and other regularization  techniques,  such  as  dropout, which we will discuss further in this notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJXWhRIdqOJ"
      },
      "source": [
        "#### Learning Rate, Batch Size, and Other Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMuxqHl1dqOJ"
      },
      "source": [
        "Here are some of the important hyperparameters other than hidden layers and neurons, and some tips on how to set them:\n",
        "\n",
        "- The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). So a  simple  approach  for  tuning  the  learning  rate  is  to  start  with  a  large  value  that makes  the  training  algorithm  diverge,  then  divide  this  value  by  3  and  try  again, and repeat until the training algorithm stops diverging.\n",
        "- Choosing  a  better  optimizer  than  plain  old  Mini-batch  Gradient  Descent  (and tuning its hyperparameters) is also quite important. We will discuss this in further sections.\n",
        "- The  batch  size  can  also  have  a  significant  impact  on  our  model’s  performance and the training time. In general the optimal batch size will be lower than 32. We will study batch normalization further in this notebook.\n",
        "- We discussed the choice of the activation function in previous assignment notebook, the $ReLU$ activation function will be a good default for all hidden layers. For the output layer, it really depends on our task.\n",
        "- In  most  cases,  the  number  of  training  iterations  does  not actually  need  to  be tweaked: just use early stopping instead.\n",
        "\n",
        "Let us also take a look at techniques such as Batch normalization, overfitting, drop out, optimizers and learning rate to  train deep neural networks.\n",
        "\n",
        "To know more about hyperparameter tuning of deep neural networks, click [here](https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_IEVbFdqOK"
      },
      "source": [
        "### Accelerate Learning of Deep Neural Networks With Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRK5Px-qdqOM"
      },
      "source": [
        "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n",
        "\n",
        "![Image](https://lh3.googleusercontent.com/-9hMD_jyPLuE/YNQoKrc-_4I/AAAAAAAACTs/9nZ-BEdtI-QoAe6R5HXlG6X3AcprX8NaQCJEEGAsYHg/s512/2021-06-23.png)\n",
        "\n",
        "$\\text{Figure: Batch Normalization Algorithm}$\n",
        "\n",
        "So  during  training,  BN  just  standardizes  its  inputs  then  rescales  and  offsets  them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_el24cRdqON"
      },
      "source": [
        "#### Implementing Batch Normalization with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmjykNm-dqOO"
      },
      "source": [
        "Implementing  Batch  Normalization  is  quite  simple, just  add  a  BatchNormalization  layer  before  or  after  each  hidden  layer’s  activation function,  and  optionally  add  a  BN  layer  as  well  as  the  first  layer  in  our  model.  For example,  this  model  applies  BN  after  every  hidden  layer  and  as  the  first  layer  in  the\n",
        "model (after flattening the input images):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUc-PjdHdqOP"
      },
      "outputs": [],
      "source": [
        "# create model with Batch Normalization\n",
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3u4zONAdqOQ"
      },
      "source": [
        "If we display the model summary, we can see that each BN layer adds 4 parameters per input: γ, β, μ and σ (for example, the first BN layer adds 3136 parameters, which is 4 times 784). The last two parameters, μ and σ, are the moving averages, they are not affected by backpropagation, so Keras calls them “Nontrainable” (if we count the total number of BN parameters, 3136 + 1200 + 400, and divide  by  two,  we get  2,368,  which  is  the  total  number  of  non-trainable  params  in this model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpyQU5LLdqOR",
        "outputId": "e5abf1ce-ce59-4b2b-ede7-5dee98d11dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 784)              3136      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 300)               235500    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 300)              1200      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 100)               30100     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 100)              400       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# summary of model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaE1qvfhdqOS"
      },
      "source": [
        "Let’s look at the parameters of the first BN layer. Two are trainable (by backprop), and two are not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTpS00QzdqOS",
        "outputId": "eb4abd0f-7679-45b9-95a0-1000f0bed1f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6zEJrikdqOT",
        "outputId": "e9747163-b322-404f-d828-2efbd79e71c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers[1].updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ku_jrTpdqOU"
      },
      "source": [
        "Moreover,  since  a  Batch  Normalization layer includes one offset parameter per input, we can remove the bias term from the previous layer (just pass `use_bias=False` when creating it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWeHUnCvdqOV"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "                    BatchNormalization(),\n",
        "                    Activation(\"relu\"),\n",
        "                    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "                    Activation(\"relu\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nQq1BuIdqOW"
      },
      "source": [
        "The  BatchNormalization class has regularizable hyperparameters. Tweaking the “momentum” argument allows us to control how much of the statistics from the previous mini batch to include when the update is calculated.\n",
        "\n",
        "\n",
        "A good momentum value is typically close to 1, for example, 0.9, 0.99, or 0.999 \n",
        "\n",
        "To know more about batch normalization, click [here](https://towardsdatascience.com/batch-normalization-in-practice-an-example-with-keras-and-tensorflow-2-0-b1ec28bde96f)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvBrjDXmdqOX"
      },
      "source": [
        "###  Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tv15JM0dqOX"
      },
      "source": [
        "Some popular optimizers used for boosting the speed in training large deep neural networks are: Momentum optimization, RMSProp, and Adam optimization. Refer [here](https://mlfromscratch.com/optimizers-explained/#/) for a detailed understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA4iM1aQdqOY"
      },
      "source": [
        "#### Momentum Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz-Y0M-SdqOZ"
      },
      "source": [
        "Momentum  optimization subtracts  the  local  gradient  from  the  momentum  vector  m  (multiplied  by  the  learning  rate  η),  and  it  updates  the  weights  by  simply  adding  this momentum vector, thus accelerating the speed. The momentum hyperparameter $β$ is introduced to prevent  the momentum from growing too large (set between 0 and 1, typically 0.9).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz1B-VDedqOa",
        "outputId": "1a26deee-4847-4cf1-bf5a-f2af8dcbc75d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#Implementing the momentum optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFp9_IkfdqOb"
      },
      "source": [
        "#### RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cWLQD2hdqOc"
      },
      "source": [
        "The RMSProp algorithm fixes only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step. \n",
        "\n",
        "The decay rate $β$ is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so we may not need to tune it at all.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ta7gsJFdqOd",
        "outputId": "c316f364-d365-460e-ad20-0a2999af46b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#Implementing the RMSProp optimizer\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1vPNNs7dqOe"
      },
      "source": [
        "#### Adam Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMvwuAridqOf"
      },
      "source": [
        "Adam combines the ideas of Momentum  optimization  and  RMSProp:  it keeps track of both, an  exponentially  decaying  average  of  past  gradients,  and  an  exponentially  decaying  average  of  past  squared  gradients.\n",
        "\n",
        "The momentum decay hyperparameter $β_1$ is typically initialized to 0.9, while the scaling  decay  hyperparameter  $β_2$  is  often  initialized  to  0.999."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EfFHwAFdqOg",
        "outputId": "4cead02c-e054-4058-a3be-5915a0261ec4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#Implementing the Adam optimizer\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYIwICgtdqOh"
      },
      "source": [
        "To know more about optimizers, click [here](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Optimizers%20are%20algorithms%20or%20methods,help%20to%20get%20results%20faster)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5i_U0udqOh"
      },
      "source": [
        "#### Learning Rate Schedule For Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ln8QNTdqOi"
      },
      "source": [
        "The simplest adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdmHxgkMdqOj"
      },
      "source": [
        "##### Time-Based Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI2I4w5ZdqOk"
      },
      "source": [
        "Keras has an in-built time-based learning rate schedule function.\n",
        "\n",
        "The decay argument in the stochastic gradient descent optimization algorithm  is used in the time-based learning rate decay schedule equation as follows:\n",
        "\n",
        "- LearningRate = LearningRate * $\\frac{1}{(1 + decay * epoch)}$\n",
        "\n",
        "When the decay argument is zero (the default), this has no effect on the learning rate.\n",
        "\n",
        "- LearningRate = 0.1 * $\\frac{1}{(1 + 0.0 * 1)} \\implies $LearningRate = 0.1\n",
        "\n",
        "When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.\n",
        "\n",
        "*See the implementation of time-based learning rate scheduling with the MNIST dataset Example at the end of this notebook.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ltSUwe4dqOu"
      },
      "source": [
        "### Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTtpElDodqOv"
      },
      "source": [
        "Deep neural networks may have millions of parameters. The network, therefore,   has vast freedom and can fit a huge variety of complex datasets. This flexibility however also makes it prone to overfitting the training set. Thus we need regularization.\n",
        "\n",
        "Let us now see some popular regularization techniques for neural networks: $ℓ1$ and $ℓ2$ regularization and dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD98NJQ6dqOw"
      },
      "source": [
        "#### $ℓ1$ and $ℓ2$ Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7-pZLbsdqOw"
      },
      "source": [
        "We can use $ℓ1$ and $ℓ2$ regularization  to  constrain  a  neural  network’s  connection  weights  (but  typically  not  its  biases).  Here  is  how  to  apply  $ℓ2$  regularization  to  a  Keras  layer’s  connection  weights, using a regularization factor of 0.01:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E369mam-dqOx"
      },
      "outputs": [],
      "source": [
        "layer = Dense(100, activation=\"relu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD9lTIltdqOy"
      },
      "source": [
        "Applying the  same  regularizer, activation function and initialization strategy repeatedly to  all  layers  in  our  network may make it error-prone. To avoid this, we can try refactoring our code to use loops. Another option is to use Python’s `functools.partial()` function: it lets us  create  a  thin  wrapper  for  any  callable,  with  some  default  argument  values.  For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IUZfTosdqOz"
      },
      "outputs": [],
      "source": [
        "# creating regularized dense layer for model\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                           activation=\"relu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwH4qCwhdqO0"
      },
      "outputs": [],
      "source": [
        "# defining model with regularization\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\",\n",
        "                     kernel_initializer=\"glorot_uniform\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9wGk2FdqO1"
      },
      "source": [
        "#### Dropout"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "23QlGnLbdqO2"
      },
      "source": [
        "Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  networks. At each training stage, individual nodes are either dropped out of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.\n",
        "\n",
        "![Image](https://i.ibb.co/HnfSTyX/M5-2.jpg)\n",
        "\n",
        "$\\text{Figure: Dropout Regularization}$\n",
        "\n",
        "To  implement  dropout  using  Keras,  we  can  use  the  keras.layers.Dropout  layer. During  training,  it  randomly  drops  some  inputs  (setting  them  to  0)  and  divides  the remaining inputs by the keep probability. After training, it just passes  the  inputs  to  the  next  layer.  For  example,  the  following  code  applies  dropout regularization before every Dense layer, using a dropout rate of 0.2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IErGvcSdqO3"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTgUd2EdqO4"
      },
      "source": [
        "If we observe that the model is overfitting, we can increase the dropout rate. Conversely, we should try decreasing the dropout rate if the model underfits the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iQC5OcP8cTx"
      },
      "source": [
        "Based on the learnings above, let us now explore hyperparameter tuning during the neural network training phase.\n",
        "\n",
        "Here, we implement the sequential model and use the **MNIST dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVb26VZQMLXn"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYcT8C3cO7Nc"
      },
      "source": [
        "We load the MNIST dataset, using Keras' dataset utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJsnHKAIMsW4",
        "outputId": "e0f597e2-9068-4af0-fcfc-bc0f8ddd99a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# data resizing variables\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHAgEhhVSJE"
      },
      "source": [
        "To feed MNIST instances into a neural network, they need to be reshaped, from a 2D image representation to a single dimension sequence. We also convert the class vector to a binary matrix (using to_categorical). This is accomplished below after which the same function defined above is called again in order to show the effects of our data reshaping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4gQx33YR1ca"
      },
      "outputs": [],
      "source": [
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBfeJhORQCyB"
      },
      "outputs": [],
      "source": [
        "# create the sequential model with BN and dropout layers\n",
        "model = Sequential([\n",
        "    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    # dropout layer to drop neurons with rate less than 0.2\n",
        "    Dropout(rate=0.2),\n",
        "    # BN layer to rescale the inputs\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    Dropout(rate=0.2),\n",
        "    Activation(\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgVUeM5yWSTS"
      },
      "source": [
        "**Note:** You can also try to define Regularized dense layer and can create a sequential model as we see in the $l1$ and $l2$ regularization section discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsKGOEeHQg_k"
      },
      "outputs": [],
      "source": [
        "# time based learning-rate scheduling\n",
        "epochs = 10\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "# define optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, decay=decay_rate)\n",
        " \n",
        "# Compile model\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ3bT-_DUpLq"
      },
      "source": [
        "**Note:** In the above code cell, you can also try compiling the with other optimizers like RMS prop, momentum optimization, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "47V1U9LhQkIJ",
        "outputId": "ebe4aba9-7728-40ac-f0e1-b4cd36544d72"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy\n",
            "\ttraining         \t (min:    0.904, max:    0.981, cur:    0.981)\n",
            "\tvalidation       \t (min:    0.959, max:    0.982, cur:    0.982)\n",
            "Loss\n",
            "\ttraining         \t (min:    0.059, max:    0.315, cur:    0.059)\n",
            "\tvalidation       \t (min:    0.066, max:    0.135, cur:    0.066)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r469/469 [==============================] - 5s 11ms/step - loss: 0.0588 - accuracy: 0.9806 - val_loss: 0.0656 - val_accuracy: 0.9818\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f047082d690>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# outputs epoch-by-epoch loss functions and accuracies at the end of each epoch of training\n",
        "plot_losses = livelossplot.PlotLossesKeras()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=epochs,\n",
        "          callbacks=[plot_losses],\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPTUDJNjVd_n"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQhlgALoVgzA",
        "outputId": "69099de4-ec94-4331-a22b-1aaa7eec72d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.06555616110563278\n",
            "Test accuracy: 0.9818000197410583\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2rywYq6bjG4"
      },
      "source": [
        "##### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhEfjsTKbny1",
        "outputId": "f0adedd1-8298-44ba-c151-aa40621c60ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_104 (Dense)           (None, 300)               235200    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 300)               0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 300)              1200      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 300)               0         \n",
            "                                                                 \n",
            " dense_105 (Dense)           (None, 100)               30000     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 100)               0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 100)              400       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_106 (Dense)           (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 267,810\n",
            "Trainable params: 267,010\n",
            "Non-trainable params: 800\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v4LSY3AdqO5"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z62bkuLOdqO5"
      },
      "source": [
        "**Q1.** How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer?\n",
        "\n",
        "**Answer 1:** Email classification is a binary classification problem, so you would only need one neuron in the output layer. This neuron would indicate the probability that the email is spam or ham. You'd most likely use the sigmoid activation function in the output layer.\n",
        "\n",
        "For the MNIST problem you would need 10 output neurons in the final layer, one for each digit. You would then replace the logistic function with the softmax function which can output one probability per class per digit.\n",
        "\n",
        "**Q2.** Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
        "\n",
        "**Answer 2:** In general, the hyperparameters of a neural network you can adjust are the number of hidden layers, the number of neurons in each hidden layer, and the activation function used by each neuron.\n",
        "\n",
        "For binary classification, use the logistic activation function. For a multi-class problem, use softmax. For a linear regression problem, don't use an activation function.\n",
        "\n",
        "Some simple ways to try and solve overfitting are reducing the number of hidden layers or the number of neurons.\n",
        "\n",
        "**Q3.** What  may  happen  if  you  set  the  momentum  hyperparameter  too  close  to  1  (e.g., 0.99999) when using an SGD optimizer?\n",
        "\n",
        "**Answer 3:** If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\n",
        "\n",
        "**Q4.** Does dropout slow down training?\n",
        "\n",
        "**Answer 4:** Yes, dropout does slow down training, in general roughly by a factor of two."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
