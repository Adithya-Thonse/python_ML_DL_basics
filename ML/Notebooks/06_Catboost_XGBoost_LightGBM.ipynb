{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"hHK2aNWllxgc"},"source":["# Catboost, XGBoost and LightGBM"]},{"cell_type":"markdown","metadata":{"id":"rPb4uAvol56w"},"source":["## Learning Objectives\n","At the end of the experiment, you will be able to :\n","\n","* perform data preprocessing\n","* perform feature transformation\n","* implement CatBoost, XGBoost and LightGBM model to perform classification using Lending Club dataset"]},{"cell_type":"markdown","metadata":{"id":"X50WsN2yl9kh"},"source":["## Introduction\n","\n","**XGBoost** was originally produced by University of Washington researchers and is maintained by open-source contributors. XGBoost is available in Python, R, Java, Ruby, Swift, Julia, C, and C++. Similar to LightGBM, XGBoost uses the gradients of different cuts to select the next cut, but XGBoost also uses the hessian, or second derivative, in its ranking of cuts. Computing this next derivative comes at a slight cost, but it also allows a greater estimation of the cut to use.\n","\n","**CatBoost** is developed and maintained by the Russian search engine Yandex and is available in Python, R, C++, Java, and also Rust. CatBoost distinguishes itself from LightGBM and XGBoost by focusing on optimizing decision trees for categorical variables, or variables whose different values may have no relation with each other (eg. apples and oranges).\n","\n","**LightGBM** is a boosting technique and framework developed by Microsoft. The framework implements the LightGBM algorithm and is available in Python, R, and C. LightGBM is unique in that it can construct trees using Gradient-Based One-Sided Sampling, or GOSS for short.\n","\n","To know more on comparisons between CatBoost, XgBoost and LightGBM, refer below\n","- [Article 1](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n","- [Article 2](https://towardsdatascience.com/catboost-vs-lightgbm-vs-xgboost-c80f40662924)"]},{"cell_type":"markdown","metadata":{"id":"0lGO-OJgmCar"},"source":["## Dataset Description\n","\n","Lending Club is a lending platform that lends money to people in need at an interest rate based on their credit history and other factors. We will analyze this data and pre-process it based on our need and build a machine learning model that can identify a potential defaulter based on his/her history of transactions with Lending Club. \n","\n","This dataset contains 42538 rows and 144 columns. **Out of these 144 columns, many columns have majorly null values.**\n","\n","To know more about the Lending Club dataset features, refer [here](https://www.openintro.org/data/index.php?data=loans_full_schema)."]},{"cell_type":"markdown","metadata":{"id":"7HLNGs_bmHvF"},"source":["###  Import required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6AyEiRD66zJ9"},"outputs":[],"source":["!pip -qq install catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7hZ-QMJcOs3"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","sns.set_style('whitegrid')\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.tree import DecisionTreeClassifier\n","from catboost import CatBoostClassifier, Pool, metrics, cv\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"JbSh_Jq2oWY1"},"source":["### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!wget https://github.com/Adithya-Thonse/python_ML_DL_basics/upload/main/ML/Datasets/LoanStats3a.zip"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import zipfile\n","with zipfile.ZipFile(\"LoanStats3a.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall(\"./\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3xYi3z2cS7v"},"outputs":[],"source":["# Load the raw loan stats dataset\n","data = pd.read_csv(\"LoanStats3a.csv\")\n","data.shape"]},{"cell_type":"markdown","metadata":{"id":"E4dwETB2_8Nl"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XE_8K_3d2n8"},"outputs":[],"source":["# View the top 5 rows of data\n","pd.set_option('display.max_columns', None)\n","\n","data.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36p-CTA3d6DB"},"outputs":[],"source":["# Size of the dataset\n","data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzpIimAdd8qu"},"outputs":[],"source":["# Checking info of the raw dataframe\n","data.info()"]},{"cell_type":"markdown","metadata":{"id":"zlFPKYZucnF4"},"source":["### Check for missing values in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BP75_3EveDF8"},"outputs":[],"source":["# Check missing values\n","data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VftRYeN1eFJi"},"outputs":[],"source":["# Total percentage of null values in the data\n","pct = (data.isnull().sum().sum())/(data.shape[0]*data.shape[1])\n","print(\"Overall missing values in the data â‰ˆ {:.2f} %\".format(pct*100))"]},{"cell_type":"markdown","metadata":{"id":"VLaMMhLJoeyI"},"source":["From above we can see that, about 63% of the values in the overall data are null values.\n","\n","Let's visualize the null values using the heatmap."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4BNkjPveZ67"},"outputs":[],"source":["# Checking for null values using a heatmap as a visualizing tool\n","plt.figure(figsize=(16,14))\n","sns.heatmap(data.isnull())\n","plt.title('Null values heat plot', fontdict={'fontsize': 20})\n","plt.legend(data.isnull())\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zURs_QrsokZX"},"source":["As we can see from the above heatmap, there are lot of null values in the dataset. We have to carefully deal with these null values."]},{"cell_type":"markdown","metadata":{"id":"OjIX4nb0haL4"},"source":["### Handling missing values in the data\n","\n","- Select columns having null values less than 40%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBImOzigeI6C"},"outputs":[],"source":["# Creating a dataframe to display percentage of null values in different number of columns\n","temp_df = pd.DataFrame()\n","temp_df['Percentage of null values'] = ['10% or less', '10% to 20%', '20% to 30%', '30% to 40%', '40% to 50%', \n","                                        '50% to 60%', '60% to 70%', '70% to 80%', '80% to 90%', 'More than 90%']\n","\n","# Store the columns count separately for each range\n","ten_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.1])\n","ten_to_twenty_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.2] & data.columns[((data.isnull().sum())/len(data)) > 0.1])\n","twenty_to_thirty_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.3] & data.columns[((data.isnull().sum())/len(data)) > 0.2])\n","thirty_to_forty_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.4] & data.columns[((data.isnull().sum())/len(data)) > 0.3])\n","forty_to_fifty_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.5] & data.columns[((data.isnull().sum())/len(data)) > 0.4])\n","fifty_to_sixty_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.6] & data.columns[((data.isnull().sum())/len(data)) > 0.5])\n","sixty_to_seventy_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.7] & data.columns[((data.isnull().sum())/len(data)) > 0.6])\n","seventy_to_eighty_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.8] & data.columns[((data.isnull().sum())/len(data)) > 0.7])\n","eighty_to_ninety_percent = len(data.columns[((data.isnull().sum())/len(data)) <= 0.9] & data.columns[((data.isnull().sum())/len(data)) > 0.8])\n","hundred_percent = len(data.columns[((data.isnull().sum())/len(data)) > 0.9])\n","\n","temp_df['No. of columns'] = [ten_percent, ten_to_twenty_percent, twenty_to_thirty_percent, thirty_to_forty_percent, forty_to_fifty_percent, \n","                             fifty_to_sixty_percent, sixty_to_seventy_percent, seventy_to_eighty_percent, eighty_to_ninety_percent, hundred_percent]\n","temp_df"]},{"cell_type":"markdown","metadata":{"id":"hHS5BcqvlQ-6"},"source":["From the above results, we can see that there are only 53 columns out of 144 columns that have null values less than 40%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mHQkFItl8rt"},"outputs":[],"source":["# Considering only those columns which have null values less than 40% in that particular column\n","df1 = data[data.columns[((data.isnull().sum())/len(data)) < 0.4]]\n","df1.shape"]},{"cell_type":"markdown","metadata":{"id":"liBwKqkZoqIR"},"source":["By considering columns with less number of null values, we were able to decrease total number of columns from 144 to 53.\n","\n","Note that we will deal with null values present in these selected 53 columns later below."]},{"cell_type":"markdown","metadata":{"id":"KJzVNT2ao7-j"},"source":["### Removing columns having single distinct value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxs3ijWIsWkT"},"outputs":[],"source":["# Checking columns that have only single values in them i.e, constant columns\n","const_cols = []\n","for i in df1.columns:\n","    if df1[i].nunique() == 1:\n","        const_cols.append(i)\n","\n","print(const_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vmDkStNezn4"},"outputs":[],"source":["# After observing the above output, we are dropping columns which have single values in them\n","print(\"Shape before:\", df1.shape)\n","df1.drop(const_cols, axis=1, inplace = True)\n","print(\"Shape after:\", df1.shape)"]},{"cell_type":"markdown","metadata":{"id":"-Lol-fG96tXl"},"source":["### Extract features from datetime columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_vwXKHACYO2"},"outputs":[],"source":["# Columns other than numerical value\n","colms = df1.columns[df1.dtypes == 'object']\n","colms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8iRbXj0DJpN"},"outputs":[],"source":["# Check which columns needs to be converted to datetime\n","df1[colms].head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRP62kpEf7ki"},"outputs":[],"source":["# Converting objects to datetime columns\n","dt_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']\n","for i in dt_cols:\n","    df1[i] = pd.to_datetime(df1[i].astype('str'), format='%b-%y', yearfirst=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-r3ih0af9sG"},"outputs":[],"source":["# Checking the new datetime columns\n","df1[['issue_d','earliest_cr_line','last_pymnt_d','last_credit_pull_d']].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrLCH2kBf_uL"},"outputs":[],"source":["# Considering only year of joining for 'earliest_cr_line' column\n","df1['earliest_cr_line'] = pd.DatetimeIndex(df1['earliest_cr_line']).year"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jZAP_xEE20T"},"outputs":[],"source":["# Adding new features by getting month and year from [issue_d, last_pymnt_d, and last_credit_pull_d] columns\n","df1['issue_d_year'] = pd.DatetimeIndex(df1['issue_d']).year  \n","df1['issue_d_month'] = pd.DatetimeIndex(df1['issue_d']).month  \n","df1['last_pymnt_d_year'] = pd.DatetimeIndex(df1['last_pymnt_d']).year  \n","df1['last_pymnt_d_month'] = pd.DatetimeIndex(df1['last_pymnt_d']).month \n","df1['last_credit_pull_d_year'] = pd.DatetimeIndex(df1['last_credit_pull_d']).year  \n","df1['last_credit_pull_d_month'] = pd.DatetimeIndex(df1['last_credit_pull_d']).month "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrE98buOgCQb"},"outputs":[],"source":["# Feature extraction\n","df1.earliest_cr_line = 2019 - (df1.earliest_cr_line)\n","df1.issue_d_year = 2019 - (df1.issue_d_year)\n","df1.last_pymnt_d_year = 2019 - (df1.last_pymnt_d_year)\n","df1.last_credit_pull_d_year = 2019 - (df1.last_credit_pull_d_year)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3TnPu4NFJSI"},"outputs":[],"source":["# Dropping the original features to avoid data redundancy\n","df1.drop(['issue_d','last_pymnt_d','last_credit_pull_d'], axis=1, inplace=True)\n","df1.shape"]},{"cell_type":"markdown","metadata":{"id":"iL0BKtxmHDsO"},"source":["### Check for missing values in reduced dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"usuwpFVdHDsP"},"outputs":[],"source":["# Checking for null values in the updated dataframe\n","plt.figure(figsize=(16,10))\n","sns.heatmap(df1.isnull())\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BOe_jJNepdO0"},"source":["### Handling Null values in reduced dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugEBAAZ0gfG-"},"outputs":[],"source":["# Checking for Percentage of null values\n","a = (df1.isnull().sum() / df1.shape[0]) * 100\n","b = a[a > 0.00]\n","b = pd.DataFrame(b, columns = ['Percentage of null values'])\n","b.sort_values(by= ['Percentage of null values'], ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9QB5SQxgiBB"},"outputs":[],"source":["# Dropping the 29 rows which have null values in few columns\n","df1 = df1[df1['delinq_2yrs'].notnull()]\n","df1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNlwQrJDJPMz"},"outputs":[],"source":["# Checking again for Percentage of null values\n","a = (df1.isnull().sum() / df1.shape[0]) * 100\n","b = a[a > 0.00]\n","b = pd.DataFrame(b, columns = ['Percentage of null values'])\n","b.sort_values(by= ['Percentage of null values'], ascending=False)"]},{"cell_type":"markdown","metadata":{"id":"oEqh3yJdJoQI"},"source":["Now, imputing the missing values with the median value for columns **'last_pymnt_d_year', 'last_pymnt_d_month', 'last_credit_pull_d_year', 'last_credit_pull_d_month', 'tax_liens'** as null values in these columns are less than 0.5% of the size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Unmqrawtg5RR"},"outputs":[],"source":["# Imputing the null values with the median value\n","df1['last_pymnt_d_year'].fillna(df1['last_pymnt_d_year'].median(), inplace=True)\n","df1['last_pymnt_d_month'].fillna(df1['last_pymnt_d_month'].median(), inplace=True)\n","df1['last_credit_pull_d_year'].fillna(df1['last_credit_pull_d_year'].median(), inplace=True)\n","df1['last_credit_pull_d_month'].fillna(df1['last_credit_pull_d_month'].median(), inplace=True)\n","# df1['tax_liens'].fillna(df1['tax_liens'].median(), inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"sC2UBoI6O2hl"},"source":["For **'revol_util'** column, filling null values with median(string) which is close to 50:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS2kqG36g8cy"},"outputs":[],"source":["# For 'revol_util' column, fill null values with 50%\n","df1.revol_util.fillna('50%', inplace=True)  \n","\n","# Extracting numerical value from string\n","df1.revol_util = df1.revol_util.apply(lambda x: x[:-1])\n","\n","# Converting string to float\n","df1.revol_util = df1.revol_util.astype('float')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gekyufGcg-6M"},"outputs":[],"source":["# Unique values in 'pub_rec_bankruptcies' column\n","df1.pub_rec_bankruptcies.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"3cwz3slEOL2J"},"source":["From the above we can see that the **'pub_rec_bankruptcies'** column is highly imbalanced. So, it is better to fill it with median(0) value as even after building model the model will be skewed very much towards 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6AWWxks-hBdW"},"outputs":[],"source":["# Fill 'pub_rec_bankruptcies' column\n","df1['pub_rec_bankruptcies'].fillna(df1['pub_rec_bankruptcies'].median(), inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SojmtM3vPS60"},"outputs":[],"source":["# Unique values in 'emp_length' column\n","df1['emp_length'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcWgdDHAhFlB"},"outputs":[],"source":["# Seperating null values by assigning a random string\n","df1['emp_length'].fillna('5000',inplace=True) \n","\n","# Filling '< 1 year' as '0 years' of experience and '10+ years' as '10 years'\n","df1.emp_length.replace({'10+ years':'10 years', '< 1 year':'0 years'}, inplace=True)\n","\n","# Then extract numerical value from the string\n","df1.emp_length = df1.emp_length.apply(lambda x: x[:2])\n","\n","# Converting it's dattype to float\n","df1.emp_length = df1.emp_length.astype('float')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9hVvsC-Qd76"},"outputs":[],"source":["# Checking again for Percentage of null values\n","a = (df1.isnull().sum() / df1.shape[0]) * 100\n","b = a[a > 0.00]\n","b = pd.DataFrame(b, columns = ['Percentage of null values'])\n","b.sort_values(by= ['Percentage of null values'], ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwKagHxGQjjV"},"outputs":[],"source":["# Removing redundant features and features which have percentage null values > 5%\n","df1.drop(['desc', 'emp_title', 'title'], axis = 1, inplace = True)\n","df1.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"-0aK6n8ipQkz"},"source":["### Converting categorical columns to numerical columns\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sNrHRi4RXr8"},"outputs":[],"source":["df1.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmWuPYAjTTEu"},"outputs":[],"source":["# Unique values in 'term' column\n","df1['term'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBl7Ure7TssE"},"outputs":[],"source":["# Unique values in 'int_rate' column\n","df1['int_rate'].unique()[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgEc_S7xgKvk"},"outputs":[],"source":["# Converting 'term' and 'int_rate' to numerical columns\n","df1.term = df1.term.apply(lambda x: x[1:3])\n","df1.term = df1.term.astype('float')\n","df1.int_rate = df1.int_rate.apply(lambda x: x[:2])\n","df1.int_rate = df1.int_rate.astype('float')\n","df1.head(2)"]},{"cell_type":"markdown","metadata":{"id":"jRoioVr0Wj_S"},"source":["Among the address related features, considering **'addr_state'** column and excluding **'zip_code'** column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFHRmxU2WjbH"},"outputs":[],"source":["df2 = df1.drop('zip_code', axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrfQzmiAgNTE"},"outputs":[],"source":["# One hot encoding on categorical columns\n","df2 = pd.get_dummies(df2, columns = ['home_ownership', 'verification_status', 'purpose', 'addr_state', 'debt_settlement_flag'], drop_first = True)\n","df2.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtwLraNPgQhN"},"outputs":[],"source":["# Label encoding on 'grade' column\n","le = LabelEncoder()\n","le.fit(df2.grade)\n","print(le.classes_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWgdfHRygSuY"},"outputs":[],"source":["# Update 'grade' column\n","df2.grade = le.transform(df2.grade)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0M83a0JgUsz"},"outputs":[],"source":["# Label encoding on 'sub_grade' column\n","le2 = LabelEncoder()\n","le2.fit(df2.sub_grade)\n","le2.classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgnXJHMMgWqY"},"outputs":[],"source":["# Update 'sub_grade' column\n","df2.sub_grade = le2.transform(df2.sub_grade)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5cFVIBtVmXT"},"outputs":[],"source":["df2.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6K3hxCUhLtW"},"outputs":[],"source":["# Target feature\n","df2['loan_status'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uWy6a5-cho6V"},"outputs":[],"source":["# Prediction features\n","X = df2.drop(\"loan_status\", axis = 1)\n","# Target variable\n","y = df2['loan_status'] \n","y.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ih7cZa2lhryV"},"outputs":[],"source":["# Label encoding the target variable\n","le3 = LabelEncoder()\n","le3.fit(y)\n","y_transformed = le3.transform(y)\n","y_transformed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EP42mHIcDAJ"},"outputs":[],"source":["X.head(2)"]},{"cell_type":"markdown","metadata":{"id":"1y50b4jgbIz8"},"source":["### Split data into training and testing set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsQy0rMJirw0"},"outputs":[],"source":["# Split the data into train and test\n","x_train, x_test, y_train, y_test = train_test_split(X, y_transformed, test_size = 0.20, stratify = y_transformed, random_state = 2)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{"id":"s_hu29-spmU9"},"source":["## Model Building"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHEAgLPWi5W5"},"outputs":[],"source":["# Using DecisionTree as base model\n","giniDecisionTree = DecisionTreeClassifier(criterion='gini', random_state = 100, \n","                                          max_depth=3, class_weight = 'balanced', min_samples_leaf = 5)\n","giniDecisionTree.fit(x_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLx7Ts_ni8M1"},"outputs":[],"source":["# Prediciton using DecisionTree\n","giniPred = giniDecisionTree.predict(x_test)\n","print('Accuracy Score: ', accuracy_score(y_test, giniPred))"]},{"cell_type":"markdown","metadata":{"id":"rt5YFRSCdno1"},"source":["### CatBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_97DlGji_RC"},"outputs":[],"source":["# Create CatBoostClassifier object\n","CatBoost_clf = CatBoostClassifier(iterations=5,\n","                                  learning_rate=0.1,\n","                                  #loss_function='CrossEntropy'\n","                                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EInDVBsmjB2p"},"outputs":[],"source":["#cat_features = list(range(0, X.shape[1]))\n","CatBoost_clf.fit(x_train, y_train,\n","                 #cat_features=cat_features,\n","                 eval_set = (x_test, y_test),\n","                 verbose = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sgR8EnjjGwe"},"outputs":[],"source":["# Prediction using CatBoost\n","cbr_prediction = CatBoost_clf.predict(x_test)\n","print('Accuracy Score: ', accuracy_score(y_test, cbr_prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8a5SLBJzlb_Y"},"outputs":[],"source":["#  Classification report for CatBoost model\n","print('Classification Report for CatBoost:')\n","print(classification_report(y_test, cbr_prediction))"]},{"cell_type":"markdown","metadata":{"id":"ObSCMRsW3paH"},"source":["### XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zs4IP_G8le9p"},"outputs":[],"source":["# Create XGBClassifier object\n","XGB_clf = XGBClassifier(learning_rate = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2ddPSXf5CBp"},"outputs":[],"source":["# Fit on training set\n","XGB_clf.fit(x_train, y_train,\n","            eval_set = [(x_train, y_train), (x_test, y_test)],\n","            verbose = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuRnPnok5SlF"},"outputs":[],"source":["# Prediction using XGBClassifier\n","XGB_prediction = XGB_clf.predict(x_test)\n","print('Accuracy Score: ', accuracy_score(y_test, XGB_prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTPyVR8k6Seh"},"outputs":[],"source":["# Classification report for XGBoost\n","\n","print('Classification Report for XGBoost:')\n","print(classification_report(y_test, XGB_prediction))"]},{"cell_type":"markdown","metadata":{"id":"x5Qr8gng6jLW"},"source":["### LightGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpMKEL4g6by0"},"outputs":[],"source":["# Create LGBMClassifier object\n","LGBM_clf = LGBMClassifier(learning_rate = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGC7j1tu68Ku"},"outputs":[],"source":["# Fit on training set\n","LGBM_clf.fit(x_train, y_train,\n","             eval_set = [(x_train, y_train), (x_test, y_test)],\n","             verbose = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_N1BW1X47E9T"},"outputs":[],"source":["# Prediction using LGBMClassifier\n","LGBM_prediction = LGBM_clf.predict(x_test)\n","print('Accuracy Score: ', accuracy_score(y_test, LGBM_prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHllXW8O7Zrz"},"outputs":[],"source":["# Classification report for LGBM\n","\n","print('Classification Report for LGBM:')\n","print(classification_report(y_test, LGBM_prediction))"]},{"cell_type":"markdown","metadata":{"id":"WEzk4g86PDEc"},"source":["## Reference Reading:\n","\n","https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Catboost_XGBoost_LightGBM.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
